\documentclass[landscape,footrule]{foils}
\usepackage[lecture-serie]{foiltex-extra}
\usepackage{crysymb}
\usepackage{graphics}
\usepackage[pdftex]{graphicx} 


\newcommand{\lserie}{LTAT.02.004 Machine Learning II}
\newcommand{\lecture}{Multivariate normal distribution\\ {\large Direct applications}}
\newcommand{\ldate}{\today}%March 26, 2019}
\newcommand{\lauthor}{Sven Laur}
\newcommand{\linst}{University of Tartu}
\MyLogo{\lserie,\ Multivariate normal distribution, \ldate}

\graphicspath{{./illustrations/}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}
\renewcommand{\VAR}{\mathbf{Var}}
\DeclareMathOperator{\diag}{diag}

\newcommand{\leqm}{\ \leq_m}


\newcommand{\bigvskip}{\vskip 2em}
\newcommand{\lastline}{\vspace*{-2ex}}
\newcommand{\spreadappart}{\vspace*{\fill}}


\newcommand{\EVPOS}{\textcolor{red}{\mathsf{evidence}^+}}
\newcommand{\EVPOSI}{\textcolor{red}{\mathsf{evidence}^+_i}}
\newcommand{\EVNEG}{\textcolor{blue}{\mathsf{evidence}^-}}

\newcommand{\COV}{\mathbf{Cov}}
\begin{document}
\titlefoil


\



\middlefoil{Important properties of\vspace*{1ex}\\ normal distributions}

\foilhead[-1cm]{Closeness under marginalisation}

Let $\vec{x}_{\III}=(x_i)_{i\in\III}$ be a subvector determined by the coordinate set $\III$.
Then $\vec{x}_{\III}$ is distributed according to a multivariate normal distribution as long as 
the vector $\vec{x}$ comes form a multivariate normal distribution $\NNN(\vec{\mu},\Sigma)$.

\begin{triangles}
\item Moment matching gives the parameters of the resulting distribution 
\begin{align*}
\EXP(\vec{x}_{\III})&= \EXP(\vec{x})_{\III}=\vec{\mu}_{\III} \\
\COV(\vec{x}_{\III})&=\COV(\vec{x})_{\III\times \III}=\Sigma[\III, \III]
\end{align*}
\end{triangles}

\foilhead[-1cm]{Closeness under linear combinations}

Linear combination $y=\vec{\alpha}_1^T \vec{x}_1+\vec{\alpha}_2^T\vec{x}_2$ of independent multivariate normal distributions $\vec{x}_1\sim\NNN(\vec{\mu}_1,\Sigma_1)$ and $\vec{x}_2\sim\NNN(\vec{\mu}_2,\Sigma_2)$ is also  a multivariate normal distribution.

\begin{triangles}
\item Moment matching gives the parameters of the resulting distribution 
\begin{align*}
\EXP(y)&= \vec{\alpha}_1^T\EXP(\vec{x}_1)+\vec{\alpha}_2^T\EXP(\vec{x}_2)=
\vec{\alpha}_1^T\vec{\mu}_1+\vec{\alpha}_2^T\vec{\mu}_2\\
\VAR(y)&=\COV(\vec{\alpha}_1^T \vec{x}_1) + \COV(\vec{\alpha}_2^T \vec{x}_2)\\
&=\vec{\alpha}_1^T \COV(\vec{x}_1)\vec{\alpha}_1+ \vec{\alpha}_2^T \COV(\vec{x}_2)\vec{\alpha}_2\\
&=\vec{\alpha}_1^T \Sigma_1\vec{\alpha}_1+ \vec{\alpha}_2^T \Sigma_2\vec{\alpha}_2
\end{align*}
\item Closeness under linear combinations holds also for matrix combinations. 
\end{triangles}

\foilhead[-1cm]{Closeness under conditioning}

Let $\vec{x}$ and $\vec{y}$ be related random variables. 
Let $\vec{x}|\vec{y}_*$ denote the conditional distribution of $\vec{x}$ given that a random variable $\vec{y}$ has a fixed value $\vec{y}_*$.
Then $\vec{x}|\vec{y}_*$ is distributed according to a multivariate normal distribution provided that 
 $(\vec{x},\vec{y})$ comes form a multivariate normal distribution $\NNN((\vec{\mu}_i),(\Sigma_{ij}))$

\begin{triangles}
\item Moment matching gives the parameters of the resulting distribution 
\begin{align*}
\EXP(\vec{x}|\vec{y}_*)&= \vec{\mu}_1+ \Sigma_{1,2}\Sigma_{2,2}^{-1}(\vec{y}-\vec{\mu}_2)\\
\COV(\vec{x}|\vec{y}_*)&= \Sigma_{1,1}-\Sigma_{1,2}\Sigma_{2,2}^{-1}\Sigma_{2,1}\\
\end{align*}
\end{triangles}


\middlefoil{Motivating examples\vspace*{1ex}\\ {\large Filtering and smoothing}}

\foilhead[-1cm]{Prediction of vector values}


Prediction errors of different vector components can be correlated.
\vspace*{3ex} 

\illustration[scale=0.5]{colored-residuals}

As a result combined model can outperform coordinatewise predictions. \vspace*{-2ex}


\foilhead[-1cm]{Sensor fusion with Hidden Markov Models}


\illustration[scale=1.0]{continuous-hidden-markov-model}

A standard problem in robotics or machine perception is following.
\begin{triangles}
\item Several sensors measure a physical system
\item Measurements are observable as $\vec{y}\in\RR^p$.
\item Physical system has an hidden state $\vec{x}\in\RR^n$.
\item Physical system evolves linearly $\vec{x}_{i+1}=A\vec{x}_i+\vec{w}_i$.
\item Measurements are linear from the state $\vec{y}_{i}=C\vec{x}_i+\vec{v}_i$.
\item Distribution of error terms $\vec{v}_i$ and $\vec{w}_i$ is known.  
\item Error terms $\vec{v}_i$ and $\vec{w}_i$ are independently drawn.
\end{triangles}



\foilhead[-1cm]{Kalman filter}

\illustration[scale=1.0]{kalman-filter}

As before we can consider the prior and filter densities
\begin{align*}
\pi[\vec{x}_i]&=p[\vec{x}_i|\vec{y}_1,\ldots, \vec{y}_{i-1}]\\
f[\vec{x}_i]&=p[\vec{x}_i|\vec{y}_1,\ldots, \vec{y}_{i}]\propto \pi[\vec{x}_i]\cdot p[\vec{y}_i|\vec{x}_i]
\end{align*}
A similar update logic assures that both distributions are normal distributions and that we can only compute the parameters of these normal distributions. 
 
\foilhead[-1cm]{Smoothing and reverse Hidden Markov Model}

\illustration[scale=1.0]{reverse-hidden-markov-model}

\begin{triangles}
\item We need likelihoods $\lambda[\vec{x}_i]=p[\vec{y}_{i+1},\ldots, \vec{y_n}|\vec{x}_i]$ for the smoothing.
\item Likelihood propagation formula is analogous to the prior propagation.
\item We can define a reverse HMM such that the prior $\pi^*[\vec{x}_i]\propto \lambda[\vec{x}_i]$.
\item The resulting HMM has reversed dynamics. 
\item It turns out that all likelihoods $\lambda[\vec{x}_i]$ are normal distributions.
\item The posterior as product $\pi[\vec{x}_i]\cdot \lambda[\vec{x}_i]\cdot p[\vec{y}_i|\vec{x}_i]$ is also a normal distribution.
\end{triangles}


\middlefoil{Motivating examples\vspace*{1ex}\\ {\large Markov fields}}




\foilhead[-1cm]{Background model for digital images}
\illustration[scale=0.8]{markov-random-field-i}

In most images intensity of pixel is influenced only by its neighbours:
\begin{triangles}
\item For simple textures the neighbourhood consist of four adjacent pixels.
\item For complex textures the the neighbourhood contains much more pixels.
\item For homogenous textures the conditional probabilities are universal\vspace*{0.5ex}.
\begin{diamonds}
\item Generative repetitive patterns for textile and grass\vspace*{1ex}   
\end{diamonds}

\item For complex patterns conditional probabilities can be location dependent.\vspace*{0.5ex}

\begin{diamonds}
\item Generative patterns for human faces and fashion accessories 
\end{diamonds}
\end{triangles}

\foilhead[-1cm]{Random Markov Fields}

\illustration[scale=1.2]{markov-random-field}

\textbf{Definition.}
Markov random field is specified by undirected graph connecting random variables $X_1,X_2,\ldots$ such that for any node $X_i$ 
\begin{align*}
\pr{x_i|(x_j)_{j\neq i}}=\pr{x_i| (x_j)_{j\in\NNN(X_i)} }
\end{align*}
where the set of neighbours $\NNN(X_i)$ is also known as \emph{Markov blanket} for $X_i$. 


\foilhead[-1cm]{Hammersley-Clifford theorem}

The probability of an observation $\vec{x}=(x_1,x_2,\ldots)$ generated by a Markov random field can be expressed in the form 
\begin{align*}
\pr{\vec{x}}=\frac{1}{Z(\omega)}\cdot\exp{-\sum_{c\in\textsf{MaxClique}}\Psi_c(\vec{x}_c,\omega)} 
\end{align*}  
where
\begin{triangles}
\item $Z(\omega)$ is a normalising constant
\item $\textsf{MaxClique}$ is the set of maximal cliques in the Markov random field
\item $\Psi_c$ is defined on the variables in the clique $c$ 
\end{triangles}
\vspace*{2ex}

The formula implies that the distribution belongs to the exponential family.

\begin{triangles}
\item Multivariate normal distribution belongs to the exponential family
\end{triangles}


\foilhead[-1cm]{Conditional Random Fields}

\illustration[scale=1.5]{conditional-random-field}

\textbf{Definition.}
Let $X_1,X_2,\ldots$ and $Y_1,Y_2,\ldots$ be random variables. The entire process is conditional random field if random variables $Y_1,Y_2,\ldots$ conditioned for any sequence of observations $x_1,x_2,\ldots$ form a Markov random field
\begin{align*}
\pr{y_i|(x_k)_{k=1}^\infty, (y_j)_{j\neq i}}=\pr{y_i|(x_k)_{k=1}^\infty, (y_j)_{j\in\NNN(Y_i)} }
\end{align*}
where the set of neighbours $\NNN(Y_i)$ is a \emph{conditional Markov blanket} for $Y_i$. 



\foilhead[-1cm]{Image segmentation and sequence labelling}
\enlargethispage{1cm}
\illustration[scale=1.0]{conditional-random-field}


\begin{triangles}
\item The input $\vec{x}$ is used to predict labels $y_1,y_2,\ldots$.
\item A correct label sequence must satisfy possibly unknown restrictions.
\item These restrictions are captured by conditional random random field.
\end{triangles}\vspace*{0.5cm}


\textbf{Consequences of Hammersley-Clifford theorem}
\begin{triangles}
\item Clique features $\Psi_c$ can depend on $(y_i)_{i\in c}$, $(x_i)_{i=1}^\infty$ 
\item Features can be defined as linear combination of vertex and edge features.
\item A vertex feature looks only variable $y_i$ associated with the vertex.
\item An edge feature looks only variables $y_i, y_j$ associated with the edge.
\end{triangles}





\middlefoil{Markov fields\vspace*{0ex}\\ with\vspace*{1ex}\\ multivariate normal distributions}



\foilhead[-1cm]{General form of the likelihood function}

The celebrated Hammersley-Clifford theorem fixes the format in which the corresponding probability distribution must be sought:
\begin{align*}
p[\vec{x}|\omega]=\frac{1}{Z(\omega)}\cdot\exp{-\sum_{c\in\textsf{MaxClique}}\Psi_c(\vec{x}_c,\omega)} 
\end{align*} 
where 
\begin{triangles}
\item $\omega$ is a set of model parameters
\item $Z(\omega)$ is a normalising constant
\item $\textsf{MaxClique}$ is the set of maximal cliques in the Markov random field
\item $\Psi_c$ is defined on the variables $x_i$ in the clique $c$. 
\end{triangles}

\foilhead[-1cm]{Multivariate normal distribution as likelihood}

If individual sub-potentials $\Psi_c(\vec{x}_c,\omega)$ are quadratic forms then the energy 
\begin{align*}
\Psi(\vec{x})=\sum_{c\in\textsf{MaxClique}}\Psi_c(\vec{x}_c,\omega) 
\end{align*} 
is also a quadratic form and thus $p[\vec{x}|\omega]$ is a multivariate normal distribution.\vspace*{1cm}

Sub-potentials are often fixed directly based on smoothness constraints
\begin{triangles}
\item Intensities have bounded variance: $\Psi_e=\delta^2 x_{ij}^2$. 
\item Intensity changes smoothly vertically: $\Psi_e=\beta(x_{i,j}-x_{i+1,j})^2$.
\item Intensity changes smoothly horizontally: $\Psi_e=\alpha(x_{i,j}-x_{i,j+1})^2$.
\end{triangles} 

\foilhead[-1cm]{Toy example}

\illustration[scale=1.0]{markov-random-field-iv}\vspace*{-0.6cm}

Sub-potentials corresponding four edges are:  
\begin{align*}
\Psi_1(x_1,x_2)&= \alpha_1(x_{1}-x_2)^2=\alpha_1 x_1^2-2\alpha_1 x_1x_2+\alpha_1 x_2^2\\
\Psi_2(x_2,x_3)&= \alpha_2(x_{2}-x_3)^2=\alpha_2 x_2^2-2\alpha_2 x_2x_3+\alpha_2 x_3^2\\
\Psi_3(x_3,x_4)&= \alpha_3(x_{3}-x_4)^2=\alpha_3 x_3^2-2\alpha_3 x_3x_4+\alpha_3 x_4^2\\
\Psi_4(x_4,x_1)&= \alpha_4(x_{4}-x_1)^2=\alpha_4 x_4^2-2\alpha_4 x_4x_1+\alpha_4 x_1^2
\end{align*}
Sub-potentials corresponding to four vertices are $\Psi_i^*(x_i)=\delta_i^2 x_i^2$ 

\foilhead[-1cm]{Resulting potential function}

\begin{align*}
\Psi(\vec{x})=\vec{x}^T
\begin{pmatrix}
\alpha_1+\alpha_4+\delta_1^2 & -\alpha_1 & 0 & -\alpha_4\\
-\alpha_1 &\alpha_1+\alpha_2+\delta_2^2 & -\alpha_2 & 0 \\
0 &-\alpha_2 &\alpha_2+\alpha_3+\delta_3^2 & -\alpha_3 \\
-\alpha_4 & 0 &-\alpha_3 &\alpha_3+\alpha_4+\delta_4^2 
\end{pmatrix}
\vec{x}
\end{align*}
and thus the covariance matrix $\Sigma$ and mean $\vec{\mu}$ can be computed by matching the shape of the multivariate normal density
\begin{align*}
p[\vec{x}|\vec{\mu},\Sigma]\propto\frac{1}{\sqrt{\det\Sigma}}\cdot\exp{-\frac{1}{2}\cdot 
(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})}
\end{align*}



\middlefoil{Motivating examples}

\foilhead[-1cm]{Hidden Markov Model}

\illustration[scale=1.0]{hidden-markov-model}

\begin{triangles}
%\item Belief propagation in Hidden Markov models is slow.
\item Inference of Hidden Markov models requires a lot of data.
\item Continuous distributions are rarely compatible with belief propagation.
\begin{align*}
\pi_{X_i}(\vec{x}_i)
&\propto \int_{\vec{x}_{i-1}} \alpha[\vec{x}_{i-1}, \vec{x}_i]\cdot\lambda_{i-1}^*(\vec{x}_{i-1})\cdot \pi_{X_{i-1}}(\vec{x}_{i-1})d\vec{x}_{i-1}\\
\lambda_{X_i}(x_i)
&\propto \int_{\vec{x}_{i+1}} \alpha[\vec{x}_{i}, \vec{x}_{i+1}]\cdot\lambda_{i}^*(\vec{x}_{i})\cdot\lambda_{X_{i+1}}(\vec{x}_{i+1})\ d\vec{x}_{i+1}
\end{align*}
\item Family of normal distributions is compatible with belief propagation.
 
\end{triangles}


\foilhead[-1cm]{Background model for digital images}
\illustration[scale=0.8]{markov-random-field-i}

In most images intensity of pixel is influenced only by its neighbours:
\begin{triangles}
\item For simple textures the neighbourhood consist of four adjacent pixels.
\item For complex textures the the neighbourhood contains much more pixels.
\item For homogenous textures the conditional probabilities are universal\vspace*{0.5ex}.
\begin{diamonds}
\item Generative repetitive patterns for textile and grass\vspace*{1ex}   
\end{diamonds}

\item For complex patterns conditional probabilities can be location dependent.\vspace*{0.5ex}

\begin{diamonds}
\item Generative patterns for human faces and fashion accessories 
\end{diamonds}
\end{triangles}

\foilhead[-1cm]{Random Markov Fields}

\illustration[scale=1.2]{markov-random-field}

\textbf{Definition.}
Markov random field is specified by undirected graph connecting random variables $X_1,X_2,\ldots$ such that for any node $X_i$ 
\begin{align*}
\pr{x_i|(x_j)_{j\neq i}}=\pr{x_i| (x_j)_{j\in\NNN(X_i)} }
\end{align*}
where the set of neighbours $\NNN(X_i)$ is also known as \emph{Markov blanket} for $X_i$. 


\foilhead[-1cm]{Hammersley-Clifford theorem}

The probability of an observation $\vec{x}=(x_1,x_2,\ldots)$ generated by a Markov random field can be expressed in the form 
\begin{align*}
\pr{\vec{x}}=\frac{1}{Z(\omega)}\cdot\exp{-\sum_{c\in\textsf{MaxClique}}\Psi_c(\vec{x}_c,\omega)} 
\end{align*}  
where
\begin{triangles}
\item $Z(\omega)$ is a normalising constant
\item $\textsf{MaxClique}$ is the set of maximal cliques in the Markov random field
\item $\Psi_c$ is defined on the variables in the clique $c$ 
\end{triangles}
\vspace*{2ex}

The formula implies that the distribution belongs to the exponential family.

\begin{triangles}
\item Multivariate normal distribution belongs to the exponential family
\end{triangles}


\foilhead[-1cm]{Conditional Random Fields}

\illustration[scale=1.5]{conditional-random-field}

\textbf{Definition.}
Let $X_1,X_2,\ldots$ and $Y_1,Y_2,\ldots$ be random variables. The entire process is conditional random field if random variables $Y_1,Y_2,\ldots$ conditioned for any sequence of observations $x_1,x_2,\ldots$ form a Markov random field
\begin{align*}
\pr{y_i|(x_k)_{k=1}^\infty, (y_j)_{j\neq i}}=\pr{y_i|(x_k)_{k=1}^\infty, (y_j)_{j\in\NNN(Y_i)} }
\end{align*}
where the set of neighbours $\NNN(Y_i)$ is a \emph{conditional Markov blanket} for $Y_i$. 



\foilhead[-1cm]{Image segmentation and sequence labelling}
\enlargethispage{1cm}
\illustration[scale=1.0]{conditional-random-field}


\begin{triangles}
\item The input $\vec{x}$ is used to predict labels $y_1,y_2,\ldots$.
\item A correct label sequence must satisfy possibly unknown restrictions.
\item These restrictions are captured by conditional random random field.
\end{triangles}\vspace*{0.5cm}


\textbf{Consequences of Hammersley-Clifford theorem}
\begin{triangles}
\item Clique features $\Psi_c$ can depend on $(y_i)_{i\in c}$, $(x_i)_{i=1}^\infty$ 
\item Features can be defined as linear combination of vertex and edge features.
\item A vertex feature looks only variable $y_i$ associated with the vertex.
\item An edge feature looks only variables $y_i, y_j$ associated with the edge.
\end{triangles}


\foilhead[-1cm]{Going beyond naive Bayesian models}
\illustration[scale=0.8]{bayesian-network}

Complex causal models are often defined through Bayesian networks
\begin{triangles}
\item A complex processes is first split into sub-events
\item Direct causal dependencies between sub-events are detected
\item Causation mechanisms are characterised with probability tables
\end{triangles} 
  
\foilhead[-1cm]{Strength and weaknesses of Bayesian networks}

\textbf{Strengths}
\begin{triangles}
\item Bayesian networks are easy to interpret
\item Bayesian networks are good for formalising fuzzy background knowledge
\item Estimation of individual probability tables is tractable
\item There are tools for doing inference with Bayesian networks  
\end{triangles}
\vspace*{1cm}

\textbf{Weaknesses}
\begin{triangles}
\item You must know the causal structure of sub-events  
\item Identification of causal structure form data alone is very difficult
\item It is notoriously difficult to model non-trivial causal dependencies
\item Standard inference procedures often do not have closed solutions 
\end{triangles}



\foilhead[-1cm]{Belief propagation. Initialisation}

\illustration[scale=1.2]{belief-propagation-in-hmm-i}
\begin{triangles}
\item We have a direct evidence $Y_i=y_i$ for each node $Y_i$. 
\item The likelihood vector is infinite and captured by $\lambda_{Y_i}=\delta_{y_i}$.
\item The local likelihood $\lambda_i^*(x_i)=p[Y_i=y_i|x_i]$ is an infinite vector. 
\item The form $\vec{y}_{i}=C\vec{x}_i+\vec{v}_i$ assures that $\vec{y}_{i}|\vec{x}_i$ is normal distribution.
\item The local likelihood $\lambda_i^*$ has a finite description.
\end{triangles}


\foilhead[-1.5cm]{Prior propagation. Filtering}
\enlargethispage{1cm}

\illustration[scale=1.2]{belief-propagation-in-hmm-ii}
\vspace*{-1.0cm}

Prior propagation rule
\begin{align*}
\pi_{X_i}(\vec{x}_i)\propto \int\limits_{\vec{x}_{i-1}} \alpha[\vec{x}_{i-1}, \vec{x}_i]\cdot\lambda_{i-1}^*(\vec{x}_{i-1})\cdot \pi_{X_{i-1}}(\vec{x}_{i-1})d\vec{x}_{i-1}
\end{align*}
leads to a finite description because on the right is a normal distribution. 



\foilhead[-1.5cm]{Likelihood propagation. Smoothing}
\enlargethispage{1cm}

\illustration[scale=1.2]{belief-propagation-in-hmm-iii}
\vspace*{-1.0cm}

Likelihood propagation rule
\begin{align*}
\lambda_{X_i}(x_i)\propto \int\limits_{\vec{x}_{i+1}} \alpha[\vec{x}_{i}, \vec{x}_{i+1}]\cdot\lambda_{X_{i+1}}(\vec{x}_{i+1})\cdot \lambda_{i}^*(\vec{x}_{i}) d\vec{x}_{i+1}
\end{align*}
leads to a finite description because on the right is a normal distribution. 

\end{document}



