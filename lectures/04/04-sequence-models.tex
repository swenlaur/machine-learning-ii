\documentclass[landscape,footrule]{foils}
\usepackage[lecture-serie]{foiltex-extra}
\usepackage{crysymb}
\usepackage{graphics}
\usepackage[pdftex]{graphicx} 




\newcommand{\lecture}{Sequence models}
\newcommand{\lserie}{LTAT.02.004 Machine Learning II}
\newcommand{\ldate}{March 16, 2021}
\newcommand{\lauthor}{Sven Laur}
\newcommand{\linst}{University of Tartu}
\graphicspath{{./illustrations/}}

\renewcommand{\vec}[1]{\boldsymbol{#1}}

\newcommand{\leqm}{\ \leq_m}

\newcommand{\EVPOS}{\textcolor{red}{\mathsf{evidence}^+}}
\newcommand{\EVNEG}{\textcolor{blue}{\mathsf{evidence}^-}}

\newcommand{\bigvskip}{\vskip 2em}
\newcommand{\lastline}{\vspace*{-2ex}}
\newcommand{\spreadappart}{\vspace*{\fill}}

\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\conf}{conf}
\DeclareMathOperator{\precision}{precision}
\DeclareMathOperator{\recall}{recall}


\begin{document}
\titlefoil


\foilhead[-1cm]{How to write a good touchscreen keyboard?}

\illustration[width=22cm]{word-prediction}

\foilhead[-1cm]{Discrete random variables}

\enlargethispage{0.9cm}
\begin{triangles}
\item A \emph{random variable} $X$ with possible \emph{outcomes} $x\in\supp(X)$
\item Compact notation for probabilities \vspace*{-1ex}
\begin{align*}
\pr{x_1}&:=\pr{\xi\gets X_1: \xi=x_1}\\
\pr{x_1\wedge x_2}&:=\pr{\xi_1\gets X_1,\xi_2\gets X_2 : \xi_1=x_1\wedge \xi_2=x_2}
\end{align*}\vspace*{-4ex}
\item Bayes formula \vspace*{-2ex}
\begin{align*}
\pr{a|b}=\frac{\pr{a\wedge b}}{\pr{b}}=\frac{\pr{b|a}\pr{a}}{\pr{b}}
\end{align*}\vspace*{-4ex}
\item Independence of random variables $X_1\ldots X_m\perp Y_1,\ldots Y_n$: \vspace*{-1ex}
\begin{align*}
 \pr{x_1\wedge\ldots\wedge x_m\wedge y_1\wedge\ldots\wedge y_n}=\pr{x_1\wedge\ldots\wedge x_m}\cdot\pr{y_1\wedge\ldots\wedge y_n}
\end{align*}\vspace*{-4ex}
\item Marginalisation over variables $Y_1,\ldots, Y_n$: \vspace*{-1ex}
\begin{align*}
 \pr{x_1\wedge\ldots\wedge x_m}=\sum_{y_1,\ldots,y_n}\pr{x_1\wedge\ldots\wedge x_m \wedge y_1\wedge\ldots\wedge y_n}
\end{align*} 
\end{triangles}


\foilhead[-1cm]{Markov chain}
\illustration[scale=1.5]{markov-chain}

\textbf{Definition.}
Let $X_1, X_2,\ldots$ be correlated random variables such that the probability of the observation $x_{i+1}$ depends only on the observation $x_{i}$.
Then the entire process is known as Markov chain.

\vspace*{1cm}

\textbf{Parametrisation.}
Markov chain is determined by specifying 

\begin{triangles}
\item state spaces $\SSS_1\ldots,\SSS_n$
\item initial probabilities $\pr{x_1}$ given as vectors 
\item state transition probabilities $\pr{x_{i+1}|x_{i}}$ given as matrices
\end{triangles}

\foilhead[-1cm]{What questions can we ask?}

\textbf{Sampling:} What are typical outcomes of the chain?\\
$\triangleright$ Synthesis of time-series, textures, sounds, games movements. 
\vspace*{1.5ex} 

\textbf{Stationary distribution:} What happens if we run the chain infinitely long?\\
$\triangleright$ Getting samples from an unnormalised posterior, optimisation tasks. 
\vspace*{1.5ex}  

\textbf{Likelihood estimation:} What is a probability of an observation $x_1,\ldots,x_n$?\\
$\triangleright$ Reasoning about probabilities and clustering sequences.
\vspace*{1.5ex} 

\textbf{Decoding:} What is the most probable outcome $x_1,\ldots,x_n$?\\ 
$\triangleright$ Imputing missing values. Rudimentary logical reasoning. 
\vspace*{1.5ex} 

\textbf{Parameter estimation:} What is are the model parameters?\\
$\triangleright$ Machine learning -- finding parameters based on observations.


\foilhead[-1cm]{Posterior maximisation in a chain}
\illustration[scale=1.3]{chain-max-i}

\textbf{Inference goal.}
Given evidence at the ends of the chain find the sequence of states $\vec{x}$ that maximise the posterior probability $\pr{\vec{x}|\mathsf{evidence}}$.
\begin{triangles}
\item The log-posterior $\log\pr{\vec{x}|\mathsf{evidence}}$ decomposes into a sum.
\item We must find a sequence with maximal weight.
\item The task can be split into subtask as all subpaths of the path with maximal weight must have maximal weight. 
\item The corresponding iterative algorithm is known as Viterbi algorithm.
\end{triangles}


\foilhead[-1cm]{Belief propagation in a chain}

\illustration[scale=1.5]{simple-chain/goal}

\textbf{Inference goal.}
Given evidence at the ends of the chain find marginal posterior probabilities for each node in the chain.
\begin{triangles}
\item Evidence $\varepsilon_V$ is an observational data associated with the node $V$.
\item Upstream $\EVPOS$ is the evidence at the beginning of chain.
\item Downstream $\EVNEG$ is the evidence at the end of chain.
\item Attributes $\pi_V, \lambda_V, p_V$ are needed to compute marginal distributions. 
\end{triangles}


\foilhead[-1cm]{Initialisation}

\illustration[scale=1.5]{simple-chain/initialisation}
\vspace*{-0.5cm}

\begin{triangles}
\item Direct evidence $\varepsilon_V$ determines the value of $V$.
\item Indirect evidence $\varepsilon_V$ determines the value distribution for $V$. 
\item We can assign the prior for the first and likelihood for the last node
\begin{align*}
\pi_A(a)&=\pr{A=a|\EVPOS}=\pr{A=a|\textcolor{red}{\varepsilon_A}}\\
\lambda_E(e)&=\pr{\EVNEG|E=e}=\pr{\textcolor{blue}{\varepsilon_E}|E=e}
\end{align*}
\end{triangles}


\foilhead[-1cm]{Belief propagation}

 
\illustration[scale=1.5]{simple-chain/belief-propagation-i}

\vspace*{-0.0cm}
\textbf{Inference goal}
\begin{align*}
\pi_B(b)&=\pr{b|\EVPOS}\\
\lambda_D(d)&=\pr{\EVNEG|d}
\end{align*}\vspace*{-1.5cm}


\textbf{Iterative propagation rules}
\begin{triangles}
\item Marginalisation gives an update rule $\lambda_D=M_{D\to E}\lambda_E$.
\item Marginalisation gives an update rule $\pi_B\propto \pi_A M_{A\to B}$.

\end{triangles}


\foilhead[-1cm]{Belief propagation}

\illustration[scale=1.5]{simple-chain/belief-propagation-ii}
\vspace*{-0.0cm}
\textbf{Inference goal}
\begin{align*}
p_C(c)=\pr{c|\EVPOS,\EVNEG}
\end{align*}\vspace*{-1.5cm}


\textbf{Iterative update rule}
\begin{triangles}
\item Bayes formula gives $p_C\propto \pi_C\otimes\lambda_C$.
\end{triangles}




\foilhead[-1cm]{Parameter inference for homogenous case}

\illustration[scale=1.5]{homogenous-markov-chain}

For a sequence of observations $\vec{x}=(x_1,\ldots, x_{n})$ the log-likelihood is
\begin{align*}
\ell[\vec{x}]&=\log \underbrace{\pr{x_1}}_{\beta[x_1]} + \sum_{i=1}^{n-1} \log \underbrace{\pr{x_{i+1}|x_{i}}}_{\alpha[x_i, x_{i+1}]} \\
&=\log \beta[x_1] + \sum_{u_1,u_2} k(u_1,u_2)\log\alpha[u_1,u_2]
\end{align*}
where $k(u_1,u_2)$ is the count of bigrams $u_1, u_2$ in the sequence $\vec{x}$.

\foilhead[-1cm]{Posterior decomposition}

\enlargethispage{0.5cm}
As a result the log-likelihood of unnormalised posterior decomposes into the sum of independent terms
\begin{align*}
\log p[\vec{\alpha},\vec{\beta}|\vec{x}]
=&\sum_{u_1} k(u_1)\log \beta[u_1] + \log p(\vec{\beta})\\
+&\sum_{u_1,u_2} k(u_1,u_2)\log\alpha[u_1,u_2]+\sum_{u_1}\log p(\vec{\alpha}[u_1,\cdot]) 
\end{align*}
where 
\begin{triangles}
\item $k(u_1)$ is the count $u_1$ at the beginning of the observed sequences
\item $k(u_1,u_2)$ is the count of bigrams $u_1, u_2$ in the observed sequences.
\item $p(\vec{\beta})$ is the prior for an entire vector of initial probabilities
\item $p(\vec{\alpha}[u_1,\cdot])$ is the prior for the transition probabilities from $u_1$ 
\end{triangles}

\foilhead[-1cm]{Reduction to the dice throwing experiment}
\enlargethispage{4cm}
Posterior decomposition leads to many independent optimisation tasks\vspace*{-2ex}
\begin{align*}
&\sum_{u_1} k(u_1)\log \beta[u_1] + \log p(\vec{\beta})\to\max\\
&\sum_{u_2} k(u_1,u_2)\log\alpha[u_1,u_2]+\log p(\vec{\alpha}[u_1,\cdot])\to\max 
\end{align*}\vspace*{-4ex} \ \\
where each of these is equivalent to optimisation of dice throwing posterior. 
Thus Maximum Aposteriori estimates for parameters are \vspace*{-2ex}
\begin{align*}
\beta[u_1]&=\frac{k(u_1)+c}{k(*)+mc} &
\alpha[u_1,u_2]&=\frac{k(u_1,u_2)+c}{k(u_1,*)+mc}
\end{align*}\vspace*{-4ex}\ \\
where
\begin{triangles}
\item $*$ is a wildcard symbol in the count queries
\item $m$ is the number of  states and $c$ is a constant for Laplacian smoothing. 
\end{triangles}

\foilhead[-1cm]{Why discrete Markov chains fail in practice?}
\enlargethispage{0.5cm}
\illustration[width=22cm, trim = 4cm 22.2cm 2cm 1.5cm, clip]{image-generation}

The number of possible observation is to big already for $8\times 8$ patch:
\begin{align*}
256^{8\times 8}\times 256^{8}\times 2^{10} = 2^{8\times 8\times 8+ 8\times 8 +10}= 2^{586}
\end{align*}
$8\times 9$ pathces are needed to estimate probabilities within $\pm 3$ percent points.

\foilhead[-1cm]{Two ways to build continious Markov chains}

\begin{triangles}
\item Replace a list of discrete states with continous variable.
 \begin{diamonds}
 \item We get $8\times 8$ input features and $8$ output features. 
 \item We need 8 functions of type $f_i:\RR^{64}\to \RR$ to fix expectation.
 \item We need 8 functions of type $g_i:\RR^{64}\to \RR$ to fix variance.
 \item If we use linear functions then we need $8\times 65\times 2$ parameters.
 \vspace{2ex} 
 \end{diamonds}
\item Embed discrete states into lower-dimensional feature space.
 \begin{diamonds}
 \item Ideally, these features are have semantical meaning.
 \item In practice, features are fixed up to affine transformations. 
 \item Thus, features do not have clear interpretation.   

 \end{diamonds}  
   
\end{triangles}

 

\foilhead[-1cm]{Hidden Markov Model}
\illustration[scale=1.5]{hidden-markov-model}

\textbf{Definition.}
Let $X_1,X_2,\ldots$ be hidden states that form a Markov chain and let $Y_1,Y_2,\ldots$ be observations that the probability of $y_i$ depends only on the state $x_i$. Then the entire process is known as Hidden Markov Model.\vspace*{1ex}

\textbf{Common tasks}
\begin{triangles}
\item parameter estimation 
\item filtering, smoothing, prediction
\end{triangles}


\foilhead[-1cm]{Applications}

\textbf{Modelling and prediction}
\begin{triangles}
\item stock prices
\item linear control algorithms 
\end{triangles}\vspace*{2ex}

\textbf{Sequence annotation}
\begin{triangles}
\item fraud detection
\item change detection 
\item functional motifs of DNA sequences
\end{triangles}\vspace*{2ex}


\textbf{Decoding}
\begin{triangles}
\item speech recognition
\item communication over a nosy channels 
\item object tracking and data fusion
\end{triangles}


\foilhead[-1cm]{Posterior maximisation in a tree}
\illustration[scale=1.3]{tree-max-i}

\textbf{Inference goal.}
Given evidence at the ends of the chain find the sequence of states $\vec{x}$ that maximise the posterior probability $\pr{\vec{x}|\mathsf{evidence}}$.
\begin{triangles}
\item The log-posterior $\log\pr{\vec{x}|\mathsf{evidence}}$ decomposes into a sum.
\item We must find a tree with maximal weight.
\end{triangles}

\foilhead[-1cm]{Decomposition into subtasks}

\illustration[scale=1.3]{tree-max-i}

All subtrees of the tree with maximal weight must have maximal weight. 
\begin{triangles}
\item We can build chains with maximum weight form leafs
\item We can merge subtrees with maximum weight to maximise the weight.
\item The algorithm works from leafs to the root node.
\item The corresponding iterative algorithm is known as Viterbi algorithm.
\end{triangles}


\foilhead[-1cm]{Belief propagation in a tree}

\illustration[scale=1.1]{simple-tree/goal}

\textbf{Inference goal.}
Given evidence at the ends of the leafs and the root of tree find marginal posterior probabilities for each node in the tree.
\begin{triangles}
\item Evidence $\varepsilon_V$ is an observational data associated with the node $V$.
\item Attributes $\pi_V, \lambda_V, p_V$ are needed to compute marginal distributions. 
\end{triangles}


\foilhead[-1cm]{Evidence decomposition}
\centerline{
\frame{\includegraphics[scale=1.1]{simple-tree/evidence-decomposition-i}}
\hspace*{1cm}
\frame{\includegraphics[scale=1.1]{simple-tree/evidence-decomposition-ii}}}

\begin{triangles}
\item Evidence decomposes into up- and downstream evidence
\item Downstream $\EVNEG(V)$ is reachable through child nodes.
\item Upstream $\EVPOS(V)$ is reachable through the predessesor node.
\item Different nodes have totally different decompositions.
\end{triangles}


\foilhead[-1cm]{Initialisation}
\enlargethispage{1cm}
\illustration[scale=0.9]{simple-tree/initialisation}
\vspace*{-0.5cm}
\textbf{Goal.} Assign prior to the root node and likelihood to the leaf nodes.
\begin{align*}
\pi_A(a)&=\pr{A=a|\EVPOS(A)}=\pr{A=a|\textcolor{red}{\varepsilon_A}}\\
\lambda_B(b)&=\pr{\EVNEG(B)|F=f}=\pr{\textcolor{blue}{\varepsilon_B}|B=b}\\
\ldots\\
\lambda_F(f)&=\pr{\EVNEG(F)|F=f}=\pr{\textcolor{blue}{\varepsilon_F}|F=f}
\end{align*}\vspace*{-4ex}\



\foilhead[-1cm]{Likelihood propagation}

\illustration[scale=0.9]{simple-tree/likelihood-propagation-i}

\vspace*{-1.0cm}
\textbf{Inference goal}
\begin{align*}
\lambda_D(d)=\pr{\EVNEG(D)|D=d}
\end{align*}
\textbf{Iterative propagation rules}
\begin{triangles}
\item Independence gives a pooling rule $\lambda_D=\lambda_1\otimes\lambda_2$
\item Marginalisation gives rules $\lambda_1= M_{D\to E}\lambda_E$ and $\lambda_2= M_{D\to F}\lambda_F$.
\end{triangles}


\foilhead[-1cm]{Posterior propagation}
\illustration[scale=0.9]{simple-tree/posterior-propagation-i}
\vspace*{-0.5cm}

\textbf{Inference goal}
\begin{align*}
p_A(a)&=\pr{A=a|\EVPOS(A),\EVNEG(A)}
\end{align*}
\textbf{Iterative propagation rule}
\begin{triangles}
\item Marginal conditional probability $p_A\propto \pi_A\otimes\lambda_A$\vspace*{-1ex}
\end{triangles}


\foilhead[-1cm]{Prior propagation}
\enlargethispage{1cm}
\illustration[scale=0.9]{simple-tree/prior-propagation-i}
\vspace*{-1.5cm}
\textbf{Inference goal}
\begin{align*}
\pi_D(d)&=\pr{D=d|\EVPOS(D)}\\
&=\pr{D=d|\EVPOS(A),\EVNEG(B),\EVNEG(C)}
\end{align*}\vspace*{-4ex}


\textbf{Iterative propagation rule}
\begin{triangles}
\item Prior can be computed as $\pi_D\propto \pi_AM_{A\to D} \otimes M_{A\to B}\lambda_B\otimes M_{A\to C}\lambda_C$\enspace.
\end{triangles}


\foilhead[-1cm]{Application on rainfall data}

\illustration[scale=0.8]{rainfall_data}
There are two monsoon seasons in Singapore: dry and wet phase.

\foilhead[-1cm]{Modelling with Hidden Markov Model}
\enlargethispage{1cm}
\illustration[scale=0.8]{hidden-markov-model}
Markov chain with states $\SSS=\set{0,1}$ and parameters
\begin{align*}
\vec{\beta}&=(0.5, 0.5)\\
\vec{\alpha}&=
\begin{pmatrix}
0.95 & 0.05\\
0.05 & 0.95\\
\end{pmatrix}
\end{align*}
Emission distributions
\begin{align*}
Y_i|X_i=0\sim\NNN(\mu_0,\sigma_0)\\
Y_i|X_i=1\sim\NNN(\mu_1,\sigma_1)\\
\end{align*}

\foilhead[-1cm]{Belief propagation. Initialisation}

\illustration[scale=1.2]{belief-propagation-in-hmm-i}
\begin{triangles}
\item We have a direct evidence $Y_i=y_i$ for each node $Y_i$. 
\item The likelihood vector is infinite and captured by $\lambda_{Y_i}=\delta_{y_i}$.
\item The local likelihood $\lambda_i^*(x_i)=\pr{Y_i=y_i|x_i}$ is a finite vector. 
\end{triangles}


\foilhead[-1.5cm]{Prior propagation. Filtering}
\enlargethispage{1cm}

\illustration[scale=1.2]{belief-propagation-in-hmm-ii}
\vspace*{-1.0cm}

Prior propagation rule yields
\begin{align*}
\pi_{X_i}(x_i)\propto \sum_{x_{i-1}\in \SSS} \alpha[x_{i-1}, x_i]\cdot\lambda_{i-1}^*(x_{i-1})\cdot \pi_{X_{i-1}}(x_{i-1})
\end{align*}
Now we can do filtering 
\begin{align*}
\pr{x_i|y_1,\ldots,y_{i}}\propto \pi_{X_i}(x_i)\cdot\lambda_i^*(x_i)
\end{align*}

\foilhead[-1.5cm]{Likelihood propagation. Smoothing}
\enlargethispage{1cm}

\illustration[scale=1.2]{belief-propagation-in-hmm-iii}
\vspace*{-1.0cm}

Likelihood propagation rule yields
\begin{align*}
\lambda_{X_i}(x_i)\propto \sum_{x_{i+1}\in \SSS} \alpha[x_{i}, x_{i+1}]\cdot\lambda_{X_{i+1}}(x_{i+1})\cdot \lambda_{i}^*(x_{i})
\end{align*}
Now we can do smoothing 
\begin{align*}
\pr{x_i|y_1,\ldots,y_{n}}\propto \pi_{X_i}(x_i)\cdot\lambda_{X_i}(x_i)
\end{align*}


\foilhead[-1cm]{Annotated rainfall data}

\illustration[scale=0.8]{rainfall_data_with_annotations}


\end{document}
