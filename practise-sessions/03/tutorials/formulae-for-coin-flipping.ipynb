{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior derivation for coin-flipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Posterior derivation for discrete uninformed prior\n",
    "\n",
    "Let us fix a uniform grid of potential bias values $p\\in\\{0.0, 0.1,\\ldots, 1.0\\}$. Then the prior probability is given by a formula\n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[p=0.0]&=\\frac{1}{11}\\\\\n",
    "\\Pr[p=0.1]&=\\frac{1}{11}\\\\\n",
    "&\\cdots\n",
    "\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "The likelihood of seeing $k$ ones out of $n$ throws is given by binomial formula\n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[k|n,p]=\\binom{n}{k}p^k(1-p)^{n-k}\n",
    "\\end{align}\n",
    "\n",
    "and thus the Bayes formula gives\n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[p|n,k]=\\frac{\\Pr[k|n,p]\\cdot \\Pr[p]}{\\Pr[k|n]}\n",
    "\\end{align}\n",
    "\n",
    "where the probability $\\Pr[k|n]$ is a apriori probability estimate that we see $k$ ones out of $n$ throws\n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[k|n]=\\sum_{p}\\Pr[k,p|n]=\\sum_p \\Pr[k|p,n]\\Pr[p]\n",
    "\\end{align}\n",
    "\n",
    "does not depend on the bias value $p$. Therefore, the probability $\\Pr[p|n,k]$  as a function of $p$ is fixed up to an unknown constant as $\\Pr[k|n,p]\\cdot \\Pr[p]$. We use the following shorthand to emphasise this fact\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[p|n,k]\\propto\\Pr[k|n,p]\\cdot \\Pr[p]\\propto p^k(1-p)^{n-k}\\enspace.\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Posterior derivation for informed prior\n",
    "\n",
    "Let us assume that we know that $p\\in[0.1,0.2]$ for the same uniform grid. Then it is straightforward to see that \n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[p]\\propto\n",
    "\\begin{cases}\n",
    "1, &\\text{if } p\\in[0.1,0.2]\\\\\n",
    "0, &\\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "and we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[p|n,k]\\propto \n",
    "\\begin{cases}\n",
    "p^k(1-p)^{n-k}, &\\text{if } p\\in[0.1,0.2]\\\\\n",
    "0, &\\text{otherwise}\\enspace.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "That is the formula is the same but restricted to the valid bias values. The latter does not mean that the posterior distributions for uninformed and informed case are the same. The unknown normalising coefficient is different for both distributions and thus the concrete values are different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Hidden assumptions behind uninformed prior\n",
    "\n",
    "Defining an uninformed prior is not so straightforward as the choice of the grid points also encodes our prefereces even if we assign the same prior probability to each model.   \n",
    "\n",
    "As an example consider an uneven grid $p\\in\\{0.0, 0.1, 0.101, 0.102, \\ldots, 0.200, 0.3,\\ldots, 1.0\\}$. Then it is straighforward to see that \n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[p\\in[0.1,0.2]]&=c\\cdot 102\\\\\n",
    "\\Pr[p\\notin[0.1,0.2]]&=c\\cdot 9 \n",
    "\\end{align}\n",
    "\n",
    "and thus we have implicitly declared that $\\Pr[p\\in[0.1,0.2]]$ is much more probable than any other parameter value.    \n",
    "\n",
    "\n",
    "This leads to a philosphical problem. What makes the uniform grid different from the non-uniform grid? There is no easy answers to it. The problem becomes even harder if the model has many alternative parametrisations. Then uniform grid in one parametrisation does not have to be uniform in the other parameterisation and we need to argue that a particular parametrisation is more natural than the others such as\n",
    "\n",
    "\\begin{align}\n",
    "pr[X=1]=p^2\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "For a coin-flipping problem the natural bias paramater $p$ is the probability of heads but for other problems the question of the most natural parametrisation is non-trivial. Information geometry gives a partial answer to it.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Posterior derivation for uninformed continous prior\n",
    "\n",
    "There are several ways to derive the posterior. We can consider the uniform grid with width $\\Delta p\\to 0$. Then $\\Pr[p|k,n]\\propto p^k(1-p)^{n-k}$ as before but the normalising constant changes\n",
    "\n",
    "\\begin{align}\n",
    "c(\\Delta p)=\\sum_{p\\in\\{0,\\Delta p,\\ldots,1\\}} p^k(1-p)^{n-k}/ \\left(\\frac{1}{\\Delta p}+1\\right)\\approx \\sum_{p\\in\\{0,\\Delta p,\\ldots,1\\}} p^k(1-p)^{n-k} \\approx\n",
    "\\int_0^1 p^k(1-p)^{n-k}dp\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "where the approximation precision becomes better and better in the process of $\\Delta p\\to 0$.\n",
    "\n",
    "One can take the integral analytically and get the correct result\n",
    "\n",
    "\\begin{align*}  \n",
    "p[p|k,n] = \\frac{\\Gamma(n+2)}{\\Gamma(k+1)\\Gamma(n-k+1)}\\cdot p^k(1-p)^{n-k}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Maximum posterior estimate\n",
    "\n",
    "If you know the posterior and can make only one guess for the bias parameter then the obvious choice is the bias value with the highest poosterior probability. Again, we can ignore constants and maximize \n",
    "\n",
    "\\begin{align}\n",
    "F(p) = p^k(1-p)^{n-k}\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The standard thechnique for maximum search is to find the derivative and equate it with zero:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial F}{\\partial p}= \\frac{\\partial p^k}{\\partial p} (1-p)^{n-k} + \\frac{\\partial (1-p)^{n-k}}{\\partial p}  p^k=0\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "The latter is doable but very technical. Hence, we observe a logarithm of the posterior.\n",
    "As the logarithm is monotone function the latter does not change the locations of maxima and minima -- by taking a logarithm we stretch and squeeze the $y$-axis wich deforms the functions but preserves the location of peaks and valleys. \n",
    "\n",
    "\\begin{align}\n",
    "\\log F &=\\log(p^k(1-p)^{n-k}) \\\\\n",
    "&= k \\log(p) + (n-k)\\log(1-p)\\\\\n",
    "\\frac{\\partial \\log F}{\\partial p}&= \n",
    "k \\cdot \\frac{1}{p} + (n-k)\\cdot \\frac{1}{1-p}\\cdot(-1) \\\\\n",
    "&= \\frac{(1-p)k -(n-k)p}{p(1-p)}=\\frac{k - np}{p(1-p)} \n",
    "\\end{align}\n",
    "\n",
    "From which we can derive\n",
    "\n",
    "\\begin{align}\n",
    "p=\\frac{k}{n}\n",
    "\\end{align}\n",
    "\n",
    "The latter is well known classical estimator for the probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Beta distribution as uninformed continous posterior\n",
    "\n",
    "A distribution with the density function\n",
    "\n",
    "\\begin{align*}  \n",
    "p[p|k,n] = \\frac{\\Gamma(n+2)}{\\Gamma(k+1)\\Gamma(n-k+1)}\\cdot p^k(1-p)^{n-k}\\enspace\n",
    "\\end{align*}\n",
    "\n",
    "is known as **beta distribution**. Beta distribution is classically parametrised with parameters \n",
    "\n",
    "\\begin{align*}\n",
    "\\alpha&=k+1\\\\\n",
    "\\beta&=n-k+1\n",
    "\\end{align*}\n",
    "\n",
    "and thus\n",
    "\\begin{align*}  \n",
    "p[p|\\alpha,\\beta] = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\cdot p^{\\alpha-1}(1-p)^{\\beta-1}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As we are not interested in the mathematical beauty of the density functions we still use $n$ and $k$ as alternative parametrisation.\n",
    "\n",
    "Beta distribution is the continous posterior of an uninformed person about the bias of the coin who observes $k$ heads out of $n$ independent throws of the same coin. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Conjugate priors and Laplace smoothing\n",
    "\n",
    "Let us now consider an experiment where we first observe $k_1$ heads out of $n_1$ throws and then observe another experiment with the same coin with $k_2$ heads out of the $n_2$ throws. Then there are two ways we can arrive to the posterior.\n",
    "\n",
    "### Single posterior update\n",
    "\n",
    "Two observations jointly can be considered as an experiment where we get $k_1+k_2$ heads out of $n_1+n_2$ throws. If we use uninformative prior then we get a beta distribution\n",
    "\n",
    "\\begin{align}\n",
    "p[p|k=k_1+k_2, n=n_1+n_2]\\propto p^{k_1+k_2}(1-p)^{n_1+n_2-k_1-k_2}\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### Iterative posterior update\n",
    "\n",
    "Alternatively we can think of it as two step experiment where we first observe $k_1$ heads out of $n_1$ throws and thus get a posterior\n",
    "\n",
    "\\begin{align}\n",
    "p[p|k=k_1, n=n_1]\\propto p^{k_1}(1-p)^{n_1-k_1}\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "To analyze the second step we need to fix a prior to the coin bias. Given our knowlegde about the previous experiment, this prior must be equal to the posterior of the first experiment. We have not received additional information and thus our uncertainty must remain the same. From this we can conclude\n",
    "\n",
    "\\begin{align}\n",
    "p[p|k=k_1+k_2, n=n_1+n_2]&\\propto p^{k_2}(1-p)^{n_2-k_2}\\cdot p^{k_1}(1-p)^{n_1-k_1}\\\\\n",
    "&\\propto p^{k_1+k_2}(1-p)^{n_1+n_2-k_1-k_2}\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "The result coincides with the first derivation. It has to be or otherwise the theory would not be **internally consitent**. We shpuld always get the same result regardles the way we decompose our problem. \n",
    "\n",
    "### Iterative belief updates and conjugate priors\n",
    "\n",
    "In many cases, we the observations arrive to us in small packets and we need to apply the Bayes rule \n",
    "\n",
    "\\begin{align}\n",
    "p[\\text{Parameters}|\\text{Data}]\\propto p[\\text{Data}|\\text{Parameters}]\\cdot p[\\text{Parameters}]\n",
    "\\end{align}\n",
    "\n",
    "several times. On the left side we product of two distributions. Normally both of them are fixed by the small set of parameters:\n",
    "\n",
    "\\begin{align}\n",
    "p[\\text{Parameters}|\\text{Data}]\\propto p[\\text{Data}|\\boldsymbol{\\alpha}]\\cdot p[\\text{Parameters}|\\boldsymbol{\\beta}]\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "For coinflipping, we have $p[\\text{Data}|\\boldsymbol{\\alpha}]$ follows Binomial distribution and $p[\\text{Parameters}|\\boldsymbol{\\beta}]$ follows beta distribution. \n",
    "\n",
    "In general, it is not guarateed that the resulting posterior distibution is a nice parametric distribution and we need to do a lot of work to get the (approximate) end result. \n",
    "\n",
    "**Ideally** the posterior distribution comes form the same parametric distribution class as the prior. If this is the case, we need to define only the update formulae for posterior distribution parameters $\\boldsymbol{gamma}$ in term of likelihood and prior parameters:\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\gamma}=f(\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}).\n",
    "\\end{align}\n",
    "\n",
    "When this ideal holds one says that the prior distribution is **conjugate** to the likelihood distribution. By our results beta distribution is conjugate prior to binomial distribution. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace smoothing\n",
    "\n",
    "Uninformed prior is not good for estimating coin bias in the practice as the maximum aposteriori estimate \n",
    "\n",
    "\\begin{align}\n",
    "p=\\frac{k}{n}\n",
    "\\end{align}\n",
    "\n",
    "can be zero or one. Zero-one estimates are bad as they make you overconfident. In practice they crash other inference algorithms by casusing division by zero errors. \n",
    "\n",
    "The problem lies in the fact that it is possible to observe exactly $0$ or $n$ heads in the cointossing experiment. \n",
    "To circumvent this we could add virtual observations. We could pretend that prior to the actual experiment we saw two coinflips: one head and one tail. \n",
    "\n",
    "More formally we fix a beta distribution as a prior distribution. As beta distrinution is a conjugate prior to binomial distribution we get a beta distribution as a posterior with \n",
    "\n",
    "\\begin{align}\n",
    "p=\\frac{k+1}{n+2}\n",
    "\\end{align}\n",
    "\n",
    "as the maximum aposteriori estimate. This is always guaranteed to be nonzero and does not create numerical problems.\n",
    "\n",
    "\n",
    "The choice of the virtual observations is subjective and we can use any values. In general Laplace smoothing is given by a formula\n",
    "\n",
    "\\begin{align}\n",
    "p=\\frac{k+\\alpha}{n+2\\alpha}\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
