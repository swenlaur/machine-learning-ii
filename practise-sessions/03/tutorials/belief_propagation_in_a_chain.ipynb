{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete derivation of belief update rules for chains\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Formal definitions\n",
    "\n",
    "### Evidence\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-evidence-i.png' width=100%>\n",
    "\n",
    "\n",
    "\n",
    "* **Direct evidence** $\\varepsilon_V$ for a node $V$ is an observation $V=v_*$ that determines the **local likelihood** $\\lambda_V^*(v)=[v=v_*]$.\n",
    "* **Indirect evidence** $\\varepsilon_V$ for a node $V$ is a partial observation that determines the **local likelihood** $\\lambda_V^*(v)=\\Pr[\\varepsilon_V|V=v]$.\n",
    "\n",
    "\n",
    "**Example:** \n",
    "* For example, an indirect observation $v\\in\\{0, 1\\}$ leads to \n",
    "\\begin{align*}\n",
    "\\lambda_v^*=\n",
    "\\begin{cases}\n",
    "1, &\\text{if } v=0,\\\\\n",
    "1, &\\text{if } v=1,\\\\\n",
    "0, &\\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evidence partitioning\n",
    " \n",
    " * **Evidence** $\\mathsf{evidence}$ is the summary evidence of all nodes in the chain. \n",
    " * **Upstream evidence** $\\color{red}{\\mathsf{evidence}^+}(V)$ is the evidence of all nodes preceeding $V$ together with the evidence for $V$.\n",
    " * **Downstream evidence** $\\color{blue}{\\mathsf{evidence}^-}(V)$ is the evidence of all nodes succeeding $V$ together with the evidence for $V$.\n",
    "\n",
    "\n",
    " <img src = '../illustrations/belief-propagation-in-chain-evidence-ii.png' width=100%>\n",
    "\n",
    "In this figure, upstream and downstream evidence for node $D$ are the following:\n",
    "\n",
    "\\begin{align*}\n",
    "\\color{red}{\\mathsf{evidence}^+}(D)  &= \\{\\varepsilon_A, \\varepsilon_C, \\varepsilon_D\\} \\\\\n",
    "\\color{blue}{\\mathsf{evidence}^-}(D) &= \\{\\varepsilon_D, \\varepsilon_G\\}\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities associated with nodes\n",
    "\n",
    "* For a node $V$ the **prior** $\\pi_V(v)=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]$.\n",
    "* For a node $V$ the **likelihood** $\\lambda_V(v)=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]$.\n",
    "* For a node $V$ the **marginal posterior probability** $p_V(v)=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V), \\color{blue}{\\mathsf{evidence}^-}(V)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Derivation of iterative update rules\n",
    "\n",
    "### Marginal posterior probabilities\n",
    "\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-marginal-posterior-i.png' width=100%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Quick prelude: Bayes formula with additional knowledge \n",
    "\n",
    "\n",
    "\n",
    "Let $a$ and $b$ be the events we want to exchange in the conditional probality and let $c$ be the additional knowledge that remains always in the side of given knowledge. Then the standard Bayes formula implies \n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[a,b,c]&=\\Pr[a|b,c]\\Pr[b,c]\\\\\n",
    "\\Pr[a,b,c]&=\\Pr[b|a,c]\\Pr[a,c]\n",
    "\\end{align}\n",
    "\n",
    "From which we can conclude \n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[a|b,c]= \\frac{\\Pr[b|a,c]\\Pr[a,c]}{\\Pr[b,c]}=\n",
    "\\frac{\\Pr[b|a,c]\\Pr[a|c]\\Pr[c]}{\\Pr[b|c]\\Pr[c]}=\\frac{\\Pr[b|a,c]\\Pr[a|c]}{\\Pr[b|c]}\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "The resulting formula\n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[a|b,c]=\\frac{\\Pr[b|a,c]\\Pr[a|c]}{\\Pr[b|c]}\n",
    "\\end{align}\n",
    "\n",
    "is known as extended Bayers formula.\n",
    "</div>\n",
    "\n",
    "\n",
    "In the following, we apply the extended Bayes formula with the following substitutions\n",
    "\\begin{align}\n",
    "a&\\equiv V=v\\\\\n",
    "b&\\equiv \\color{blue}{\\mathsf{evidence}^-}(V)\\\\\n",
    "c&\\equiv \\color{red}{\\mathsf{evidence}^+}(V)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The resulting mechanical application of Bayes rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "p_V(v)\n",
    "&= \\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V),\\color{blue}{\\mathsf{evidence}^-}(V)]\\\\\n",
    "&=\\frac{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v,\\color{red}{\\mathsf{evidence}^+}(V)]\n",
    "  \\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]}{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|\\color{red}{\\mathsf{evidence}^+}(V)]}\\\\\n",
    "&\\propto \\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v,\\color{red}{\\mathsf{evidence}^+}(V)]\n",
    "  \\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As direct knowledge of the state $V=v$ completely determines what happens with the next node, the knowledge $V=v,\\color{red}{\\mathsf{evidence}^+}(V)$ is equivalent to the knowledge $V=v$ and we can simplify:\n",
    "\n",
    "\\begin{align*}\n",
    "p_V(v)\n",
    "&\\propto \\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\n",
    "  \\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&\\propto \\lambda_V(v)\\cdot \\pi_V(v)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As a result, if we know the likelihood $\\lambda_V(\\cdot)$ and posterior $\\pi_V(\\cdot)$ up to a constant then we can recover the marginal posterior $p_V(\\cdot)$ through normalisation. \n",
    "Up to a constant in this context means that we can omit all factors that do not depend on the value $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood update for a node without evidence\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-likelihood-i.png' width=100%>\n",
    "\n",
    "Let $W$ be a successor node of $V$, then mechanical application of marginalisation rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\sum_{w\\in W}\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)\\wedge W=w|V=v]\\\\\n",
    "&=\\sum_{w\\in W}\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|W=w,V=v]\\cdot \\Pr[W=w|V=v] \\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As the node $V$ has no evidence, the downstream evidence must be in the successor nodes and thus $\\color{blue}{\\mathsf{evidence}^-}(V)= \\color{blue}{\\mathsf{evidence}^-}(W)$. \n",
    "The Markov property assures that knowledge of $V=v$ is redundant when we know $W=w$. \n",
    "Consequently, we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\sum_{w\\in W}\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|W=w]\\cdot \\Pr[W=w|V=v] \\\\\n",
    "&=\\sum_{w\\in W}\\lambda_W(w)M_{V\\to W}[v, w] \\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Representing $\\lambda_W(\\cdot)$ as a column vector allows us to compact the equation in matrix notation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V\\propto M_{V\\to W} \\lambda_W\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood update for a node with direct evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-likelihood-ii.png' width=100%>\n",
    "\n",
    "Let $W$ be a successor node of $V$ and let $V=v_*$ be a direct evidence associated with the node $V$.\n",
    "Then evidence decomposition yieds\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)\\wedge V=v_*|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|V=v_*,V=v]\\cdot \\Pr[V=v_*|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|V=v_*]\\cdot [v_*=v]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\lambda_V(v)$ is nonzero only for a single value $v_*$. \n",
    "Thus by multiplying $\\lambda_V(v)$ with a constant value $\\lambda_V(v_*)^{-1}$, we get an indicator:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\\propto [v=v_*]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that  $\\lambda_V(v_*)^{-1}$ depends on $v_*$ but remains constant if we consider different values of $v\\in V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood update for a node with indirect evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-likelihood-ii.png' width=100%>\n",
    "\n",
    "Let $W$ be a successor node of $V$ and let $\\varepsilon_V$ be an indirect evidence associated with the node $V$.\n",
    "Then evidence decomposition yieds\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)\\wedge \\varepsilon_v|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|\\varepsilon_V,V=v]\\cdot \\Pr[\\varepsilon_V|V=v]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Again the direct knowledge $V=v$ subsumes the partial knowledge $\\varepsilon_V$ and we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|V=v]\\cdot \\lambda_V^*(v)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda_V^*(v)=\\Pr[\\varepsilon_V|V=v]$ is the local likelihood.\n",
    "Now there can be several states for which the likelihood is nonzero and thus we must separately compute the left term.\n",
    "Again the mechanical application of the marginalisation rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_1(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|V=v]\\\\\n",
    "&=\\sum_{w\\in W} \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)\\wedge W=w|V=v]\\\\\n",
    "&=\\sum_{w\\in W} \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|W=w, V=v]\\cdot \\Pr[W=w|V=v]\\\\\n",
    "&=\\sum_{w\\in W} \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W)|W=w]\\cdot M_{V\\to W}[v,w]\\\\\n",
    "&=\\sum_{w\\in W} \\lambda_W(w)\\cdot M_{V\\to W}[v,w] \\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Thus matrix algebra allows us to compact the update rule:\n",
    "\\begin{align*}\n",
    "\\lambda_1 &= M_{V\\to W}\\lambda_W\\\\\n",
    "\\lambda_V &= \\lambda_1\\otimes \\lambda_V^*\n",
    "\\end{align*}\n",
    "where $\\otimes$ represents pointwise multiplication of vector entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood update for a node without successors\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-likelihood-iii.png' width=100%>\n",
    "\n",
    "\n",
    "The rules for updating the likelihood are applicable for nodes that do have successors.\n",
    "Hence, we must address nodes without successors explicitly. \n",
    "Without loss of generality, we can assume that for such a node $V$ there is evidence $\\varepsilon_V$. If the evidence is missing we can treat this as a partial observation $v\\in V$ that creates a local likelihood $\\lambda_V^*=1$. If the evidence is direct then it creates a local likelihood $\\lambda_V^*(v)=[v=v_*]$.  \n",
    "As a result, we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\Pr[\\varepsilon_V|V=v]\\\\\n",
    "&=\\lambda_V^*(v)\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update  for a node without evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-prior-i.png' width=100%>\n",
    "\n",
    "Let $U$ be the predcessor node of $V$ then mechanical application of marginalisation rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v\\wedge U=u|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v|U=u,\\color{red}{\\mathsf{evidence}^+}(V)]\\cdot \\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(V)]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As the node $V$ has no evidence, the upstream evidence must be in the predecessor nodes and thus $\\color{red}{\\mathsf{evidence}^+}(V)= \\color{red}{\\mathsf{evidence}^+}(U)$. \n",
    "The Markov property assures that knowledge of $\\color{red}{\\mathsf{evidence}^+}(U)$ is redundant when we know $U=u$. \n",
    "Consequently, we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\sum_{u\\in U}\\Pr[V=v|U=u]\\cdot \\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&=\\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\pi_U(u)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Representing $\\pi_U(\\cdot)$ as a row vector allows us to compact the equation in matrix notation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V\\propto \\pi_U M_{U\\to V}\\enspace.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update  for a node with direct evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-prior-ii.png' width=100%>\n",
    "\n",
    "Let $U$ be a predecessor node of $V$ and let $V=v_*$ be a direct evidence associated with the node $V$.\n",
    "Then the evidence decoposition yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\Pr[V=v|V=v_*,\\color{red}{\\mathsf{evidence}^+}(U)]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Again the evidence $V=v_*$ is the most direct information about $V$, the remaining evidence $\\color{red}{\\mathsf{evidence}^+}(U)$ is irrelevant unless $\\color{red}{\\mathsf{evidence}^+}(U)$ directly contradicts $V=v_*$.\n",
    "In this case, nothing can be done and prior is not defined at all.\n",
    "Thus we can simplify and get an indicator prior:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|V=v_*]\\\\\n",
    "&=[v=v_*]\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update  for a node with indirect evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-prior-ii.png' width=100%>\n",
    "\n",
    "Let $U$ be a predecessor node of $V$ and let $\\varepsilon_V$ be an indirect evidence associated with the node $V$.\n",
    "Then the evidence decoposition yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\Pr[V=v|\\varepsilon_V,\\color{red}{\\mathsf{evidence}^+}(U)]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Mechanical application of Bayes rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\varepsilon_V,\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&=\\frac{\\Pr[\\varepsilon_V|V=v,\\color{red}{\\mathsf{evidence}^+}(U)]\\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(U)]}{\\Pr[\\varepsilon_V|\\color{red}{\\mathsf{evidence}^+}(U)]}\\\\\n",
    "&\\propto\\Pr[\\varepsilon_V|V=v,\\color{red}{\\mathsf{evidence}^+}(U)]\\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(U)]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As $\\color{red}{\\mathsf{evidence}^+}(U)$ is redundant if we know $V=v$, we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto\\Pr[\\varepsilon_V|V=v]\\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&\\propto\\lambda_V^*(v)\\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(U)]\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda_V^*(v)=\\Pr[\\varepsilon_V|V=v]$ is the local likelihood. \n",
    "Let $\\pi_1(v)$ denote the second factor. Then mechanical application of the marginalisation rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_1(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v\\wedge U=u|\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v|U=u,\\color{red}{\\mathsf{evidence}^+}(U)]\\cdot\\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v|U=u]\\cdot\\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&=\\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot\\pi_U(u)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Thus matrix algebra allows us to compact the update rule:\n",
    "\\begin{align*}\n",
    "\\pi_1 &= \\pi_U M_{U\\to V}\\\\\n",
    "\\pi_V &= \\pi_1\\otimes \\lambda_V^*\n",
    "\\end{align*}\n",
    "where $\\otimes$ represents pointwise multiplication of vector entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update for a node without  a predecessor\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-chain-prior-iii.png' width=100%>\n",
    "\n",
    "\n",
    "The rules for updating the prior are applicable for nodes that do have predecessors.\n",
    "Hence, we must address nodes without predecessors explicitly. \n",
    "Without loss of generality, we can assume that for such a node $V$ there is evidence $\\varepsilon_V$. If the evidence is missing we can treat this as a partial observation $v\\in V$ that creates a local likelihood $\\lambda_V^*=1$. If the evidence is direct then it creates a local likelihood $\\lambda_V^*(v)=[v=v_*]$. \n",
    "As a result, we get an expression that can be further manipulated with the Bayes rule:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\Pr[V=v|\\varepsilon_V]\\\\\n",
    "&=\\frac{\\Pr[\\varepsilon_V|V=v]\\cdot \\Pr[V=v]}{\\Pr[\\varepsilon_V]}\\\\\n",
    "&\\propto\\lambda_V^*(v)\\cdot M_{V}[v]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $M_V$ is the vector of initial probabilities. The corresponding matrix formulation is\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V\\propto \\lambda_V^*\\otimes M_V\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "For a direct observation the expression simplifies to \n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V\\propto [v=v_*]\n",
    "\\end{align*}\n",
    "\n",
    "as only one entry is nonzero.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
