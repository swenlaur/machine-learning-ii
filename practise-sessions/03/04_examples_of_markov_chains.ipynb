{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Markov Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import numpy.random as rnd\n",
    "import string\n",
    "\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from typing import List\n",
    "\n",
    "from tqdm import tnrange\n",
    "from plotnine import *\n",
    "\n",
    "# Local imports\n",
    "from common import *\n",
    "from convenience import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Markov chain as a language model \n",
    "There are many methods for detecting the language of a text document. \n",
    "The simplest one is based on Markov cains of order one, i.e., the last letter determines the probabilities for the next letter.\n",
    "It is terribly naive but it is useful for language detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Likelihood and data generation\n",
    "\n",
    "Let $\\beta[x]$ denote the probability that the first letter is $x$. Let $\\alpha[x,y]$ denote the probability that the next letter is $y$ provided that the last letter was $x$.\n",
    "Then we can easily estimate the probability that a word $\\boldsymbol{x} = (x_0,\\ldots, x_n)$ came from this distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[\\boldsymbol{x}|\\alpha,\\beta]= \\beta[x_0]\\cdot\\prod_{i=1}^n \\alpha[x_{i-1},x_i]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As an example, let us define a likelihood for a truly random lower-case language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlang_likelihood(x:string) -> float:\n",
    "    alphabet = list(string.ascii_lowercase)\n",
    "    alpha = (combine_categories({'x': alphabet, 'y': alphabet})\n",
    "             .assign(pr = lambda df: 1/len(df))\n",
    "             .set_index(['x', 'y']))  \n",
    "    beta = (DataFrame({'x': alphabet})\n",
    "        .assign(pr = lambda df: 1/len(df))\n",
    "        .set_index(['x']))\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        return 1\n",
    "    \n",
    "    pr = beta.loc[x[0], 'pr']\n",
    "    for i in range(1, len(x)):\n",
    "        pr *= alpha.loc[(x[i-1], x[i]), 'pr']\n",
    "    \n",
    "    return pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.416533573215762e-08\n",
      "8.416533573215762e-08\n"
     ]
    }
   ],
   "source": [
    "print(rlang_likelihood('abs'))\n",
    "print(rlang_likelihood('sss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rlang_gen(n:int) -> str:\n",
    "     alphabet = np.array(list(string.ascii_lowercase))\n",
    "     return ''.join(list(rnd.choice(alphabet, n, replace=True)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eopdt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlang_gen(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Naive parameter estimation\n",
    "\n",
    "The parameters of the Markov Chain can be computed by looking at relative frequencies of start symbols and bigrams in words. The naive maximum likelihood estimates are the following:\n",
    "\n",
    "\\begin{align*}\n",
    " \\beta[x]&=\\frac{\\# \\text{words starting with }x}{\\# \\text{words}}\\\\\n",
    " \\alpha[x,y]&=\\frac{\\# \\text{bigrams of }xy}{\\# \\text{bigrams starting with }x}\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Language detection without Laplace smoothing (<font color='red'>1p</font>)\n",
    "\n",
    "Use files `est_training_set.csv` and `eng_training_set.csv` in the directory `data` to learn model parameters $\\alpha$ and $\\beta$ for both languages using maximum likelihood estimates.\n",
    "Put these parameters into the formal model to compute probabilities\n",
    "\n",
    "\\begin{align*}\n",
    "      p_1 &=\\Pr[word|\\mathsf{Estonian}]\\\\\n",
    "      p_2 &=\\Pr[word|\\mathsf{English}]\n",
    "\\end{align*}\n",
    "\n",
    "and then use Bayes formula\n",
    "\n",
    "\\begin{align*}\n",
    " \\Pr[\\mathsf{Estonian}|word]\n",
    " =\\frac{\\Pr[word|\\mathsf{Estonian}]\\Pr[\\mathsf{Estonian}]}{\\Pr[word]}\n",
    "\\end{align*}\n",
    "to guess the language of a word on test samples `est_test_set.csv` and `eng_test_set.csv`.\n",
    "Why the procedure does not work? \n",
    "\n",
    "**Hint:** The number of samples is not the problem. You can assume that there are enough samples to estimate all parameters with high accuracy. The same problem could have manifested even if there were millions of word examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Language detection with Laplace smoothing (<font color='red'>1p</font>)\n",
    "\n",
    "Use files `est_training_set.csv` and `eng_training_set.csv` in the directory `data` to learn model parameters $\\alpha$ and $\\beta$ for both languages using Laplace smoothing.\n",
    "Put these parameters into the formal model to compute probabilities\n",
    "\n",
    "\\begin{align*}\n",
    "      p_1 &=\\Pr[word|\\mathsf{Estonian}]\\\\\n",
    "      p_2 &=\\Pr[word|\\mathsf{English}]\n",
    "\\end{align*}\n",
    "\n",
    "and then use Bayes formula\n",
    "\n",
    "\\begin{align*}\n",
    " \\Pr[\\mathsf{Estonian}|word]\n",
    " =\\frac{\\Pr[word|\\mathsf{Estonian}]\\Pr[\\mathsf{Estonian}]}{\\Pr[word]}\n",
    "\\end{align*}\n",
    "to guess the language of a word on test samples `est_test_set.csv` and `eng_test_set.csv`.\n",
    "Did the problem disappear? If not, what we have to consider if we apply Laplace smoothing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
