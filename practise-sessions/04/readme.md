## Home exercises

### Concepts to learn:

* Finding parameters of Markov chains from training data
  * Maximum-likelihood estimates for starting and transition probabilities
  * Laplace smoothing as a way to avoid undesired impossible events
* Prediction and imputation of missing values with Markov Chains
  * Decoding as a way to prediction for a sequence of states
  * Belief propagation as a way to give independent predictions for each state
* Finding parameters of higher-order Markov chains from training data
  * Training data scarcity problem
  * Image simulation with Hilbert curves   
* How one can cheat to find parameters of Hidden Markov Model (HMM)
  * How to determine emission probabilities to model typing errors
  * How to use correct texts to learn starting and transition probabilities
* Prediction and imputation of missing values with Hidden Markov Models
  * Decoding as a way to prediction for a sequence of hidden states
  * Belief propagation as a way to give independent predictions for each hidden state
* Hidden Markov Model as a segmentation and labelling tool
  * Using normal distributions to define emission probabilities
  * Markov model as way to specify allowed transitions between underspecified states


### Points to score
Home exercises are scattered between the files. Starred exercises cover interesting optional aspects.

* 01_markov_chains.ipynb
* 02_higher_order_markov_chains.ipynb
* 03_timeseries_as_markov_chains.ipynb
* 04_markov_models_in_image_processing.ipynb
* 05_markov_chains_with_continous_state_space.ipynb
* 06_hidden_markov_models.ipynb
* 07_change_detection_with_hidden_markov_models.ipynb
* 08_hidden_markov_models_with_continous_state_space.ipynb
* 09_regime_changes_in_covid-19_pandemia.ipynb
