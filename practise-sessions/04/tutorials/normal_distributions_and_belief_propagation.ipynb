{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200e4cf8",
   "metadata": {},
   "source": [
    "# Normal distribution and belief propagation \n",
    "\n",
    "In the following we derive belief propagation formulae for Hidden Markov Models where the states $x_i$ and the observations $y_i$ are continuous real variables and the correspondence between them are governed by linear equations \n",
    "\n",
    "\\begin{align*}\n",
    "x_{i+1}&=ax_i + b +w_i\\\\\n",
    "y_i&=cx_i+ d +v_i \n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "* the initial state $x_0$ is fixed;\n",
    "* state disturbances $w_i$ is modelled by a normal distribution $\\mathcal{N}(0, \\rho_i)$;\n",
    "* measurement noise $v_i$ is modelled by a normal distribution $\\mathcal{N}(0, \\tau_i)$;\n",
    "* all random variables  $w_1, \\ldots, w_n, v_1, \\ldots, v_n$ are assumed to be independent.\n",
    "\n",
    "\n",
    "The setup where the states and observations are vectors of real numbers and the update rules are vector equations is analogous but highly technical and thus omitted from this tutorial. \n",
    "To simplify the treatment, we introduce couple facts about univariate and multivariate normal distributions. \n",
    "\n",
    "\n",
    "### Closeness under linear combinations\n",
    "\n",
    "Linear combination $v=\\alpha_1 u_1+\\alpha_2 u_2+\\cdots+\\alpha_n u_n$ of independent univariate normal distributions $u_1\\sim\\mathcal{N}(\\mu_1,\\sigma_1), \\ldots,u_n\\sim\\mathcal{N}(\\mu_n,\\sigma_n)$ is also a normal distribution $\\mathcal{N}(\\mu, \\sigma)$ where parameters $\\mu$ and $\\sigma$ can be determined with moment matching.\n",
    "\n",
    "\n",
    "### Closeness under conditioning with linear constraints\n",
    "\n",
    "Let $\\boldsymbol{x}$ be distributed according to multivariate normal distribution. Let $A\\boldsymbol{x}=\\boldsymbol{b}$ be a linear constraint on $\\boldsymbol{x}$. Then the conditional distribution \n",
    "$p[\\boldsymbol{x}|A\\boldsymbol{x}=\\boldsymbol{b}]$ can be expressed as a multivariate normal distribution, i.e., $\\boldsymbol{x}|A\\boldsymbol{x}=\\boldsymbol{b}$ is distributed according to multivariate normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad6a16",
   "metadata": {},
   "source": [
    "## I. Known facts about normal distributions \n",
    "\n",
    "To simplify the treatment, we introduce couple facts about univariate and multivariate normal distributions. \n",
    "\n",
    "\n",
    "### Closeness under linear combinations\n",
    "\n",
    "Linear combination $v=\\alpha_1 u_1+\\alpha_2 u_2+\\cdots+\\alpha_n u_n$ of independent univariate normal distributions\n",
    "\n",
    "\\begin{align*}\n",
    "u_1&\\sim\\mathcal{N}(\\mu_1,\\sigma_1)\\\\\n",
    "u_2&\\sim\\mathcal{N}(\\mu_2,\\sigma_2)\\\\\n",
    "\\cdots&\\sim\\cdots\\\\\n",
    "u_n&\\sim\\mathcal{N}(\\mu_n,\\sigma_n)\n",
    "\\end{align*}\n",
    "\n",
    "is also a normal distribution $\\mathcal{N}(\\mu, \\sigma)$ where parameters $\\mu$ and $\\sigma$ can be determined with moment matching.\n",
    "\n",
    "\n",
    "### Closeness under conditioning with linear constraints\n",
    "\n",
    "Let $\\boldsymbol{x}$ be distributed according to multivariate normal distribution. Let $A\\boldsymbol{x}=\\boldsymbol{b}$ be a linear constraint on $\\boldsymbol{x}$. Then the conditional distribution \n",
    "$p[\\boldsymbol{x}|A\\boldsymbol{x}=\\boldsymbol{b}]$ can be expressed as a multivariate normal distribution, i.e., $\\boldsymbol{x}|A\\boldsymbol{x}=\\boldsymbol{b}$ is distributed normally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3712493",
   "metadata": {},
   "source": [
    "## II. Belief propagation for Markov chains\n",
    "\n",
    "Let us first consider the Markov chain with real-valued state that evolves according to the affine update rule \n",
    "\n",
    "\\begin{align*}\n",
    "x_{i+1}=a x_{i} + b + w_i \n",
    "\\end{align*}\n",
    "\n",
    "where the initial state $x_0$ is a fixed number and the error terms satisfy the following restrictions:\n",
    "\n",
    "* all error terms $w_i$ are independent;  \n",
    "* each error term $w_i$ is distributed according $\\mathcal{N}(0,\\sigma_i)$.\n",
    "\n",
    "**First observation.** The state $x_1$ is distributed according to the normal distribution as an affine transformation of $w_1$. \n",
    "\n",
    "\n",
    "**Second observation.** If the state $x_i$ is distributed according to the normal $\\mathcal{N}(\\mu_i,\\rho_i)$ distribution then the closeness under linear combinations assures that $x_{i+1}$ is also distributed as a normal distribution $\\mathcal{N}(\\mu_{i+1},\\rho_{i+1})$. \n",
    "This guarantees that all states $x_{1}, \\ldots, x_n$ are distributed according to normal distribution.\n",
    "\n",
    "\n",
    "In principle, it is possible to derive density function for $x_{i+1}$ directly as\n",
    "\n",
    "\\begin{align}\\tag{P1} \n",
    "\\pi[x_{i+1}]&=\\int\\limits_{-\\infty}^\\infty p[x_i|x_0]\\cdot p[x_{i+1}|x_i]\\cdot dx_i = \\int\\limits_{-\\infty}^\\infty \\pi[x_i]\\cdot p[x_{i+1}|x_i]\\cdot dx_i\\enspace. \n",
    "\\end{align}\n",
    "\n",
    "However, the resulting integral\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi[x_{i+1}]&=\\frac{1}{2\\pi\\rho_i\\sigma_i}\\cdot\\int\\limits_{-\\infty}^\\infty \n",
    "\\exp\\left(-\\frac{(x_i-\\mu_i)^2}{2\\rho_i^2}\\right)\\cdot \\exp\\left(-\\frac{(x_{i+1}-ax_i-b)^2}{2\\sigma_i^2}\\right)\\cdot dx_i\n",
    "\\end{align*}\n",
    "\n",
    "is technically hard compute and it is much simpler to use moment matching\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{E}(x_{i+1})&=\\mathbf{E}(ax_{i}+b+w_i)=a\\mu_i\\\\\n",
    "\\mathbf{D}(x_{i+1})&=\\mathbf{D}(ax_{i}+b+w_i)=a^2\\rho_i^2+\\sigma_i^2\\enspace\n",
    "\\end{align*}\n",
    "\n",
    "to derive the parameters of the resulting normal distribution\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{i+1}  &=a\\mu_i\\\\\n",
    "\\rho_{i+1} &=\\sqrt{a^2\\rho_i^2+\\sigma_i^2}\\enspace. \n",
    "\\end{align*}\n",
    "\n",
    "In other words, the prior propagation can be done by iteratively updating the parameters $\\mu_i$ and $\\rho_i$ from start of the chain to the end of the chain. \n",
    "For non-homogenous Markov chains, the update rule is just varies from node to node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20bf82",
   "metadata": {},
   "source": [
    "## III. Likelihood propagation and a reverse chain \n",
    "\n",
    "The second essential component of belief propagation is likelihood propagation through the formula\n",
    "\n",
    "\\begin{align}\\tag{L1}\n",
    "  \\lambda[x_{i}] = p[x_n|x_i]= \\int\\limits_{-\\infty}^\\infty p[x_n|x_{i+1}]\\cdot p[x_{i+1}|x_i]\\cdot dx_{i+1}= \\int\\limits_{-\\infty}^\\infty \\lambda[x_{i+1}]\\cdot p[x_{i+1}|x_i]\\cdot dx_{i+1}\n",
    "\\end{align}\n",
    "\n",
    "which is surprisingly similar to the prior propagation formula. In fact, we can define a reverse Markov chain \n",
    "\n",
    "\n",
    "with carefully crafted transition probabilities $q[x_{i}|x_{i+1}]$ so that the corresponding prior $\\pi^*[x_i]=c_i\\lambda[x_i]$. \n",
    "For that it is sufficient \n",
    "\n",
    "\\begin{align}\\tag{R1}\n",
    "   c_i\\cdot\\int\\limits_{-\\infty}^\\infty \\pi^*[x_{i+1}]\\cdot p[x_{i+1}|x_i]\\cdot dx_{i+1}= c_{i}c_{i+1}\\lambda[x_i]=\n",
    "   c_{i+1}\\cdot\\int\\limits_{-\\infty}^\\infty \\pi^*[x_{i+1}]\\cdot q[x_i|x_{i+1}]\\cdot dx_{i+1}\\enspace\n",
    "\\end{align}\n",
    "\n",
    "which is clearly satisfied when we define \n",
    "\n",
    "\\begin{align}\\tag{R2}\n",
    " q[x_i|x_{i+1}]=\\propto p[x_{i+1}|x_i]\\enspace\n",
    "\\end{align}\n",
    "\n",
    "where the hidden coefficient just normalises density $q[x_i|x_{i+1}]$. \n",
    "Now note that \n",
    "\n",
    "\\begin{align*}\n",
    "p[x_{i+1}|x_i]\\propto \\exp\\left( -\\frac{(x_{i+1}-ax_i-b)^2}{2\\sigma_i^2}\\right)\n",
    "= \\exp\\left( -\\frac{\\left(x_i-\\frac{x_{i+1}}{a}+\\frac{b}{a}\\right)^2}{2\\left(\\frac{\\sigma_i}{a}\\right)^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "which indicates that $q[x_i|x_{i+1}]$ can be defined as a normal distribution $\\mathcal{N}(\\mu_i^*, \\sigma_i^*)$ with parameters\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_i^*&= \\frac{x_{i+1}}{a}-\\frac{b}{a}\\\\\n",
    "\\sigma^*_i&=\\frac{\\sigma_i}{a}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the affine update rule\n",
    "\n",
    "\\begin{align*}\n",
    "x_i=\\frac{x_{i+1}+b-w_i}{a_i}\n",
    "\\end{align*}\n",
    "\n",
    "gives exactly the same denisity  and thus we can compute the likelihood as prior in the reverse chain. \n",
    "Let the corresponding distribution $\\pi^*[x_i]$ be denoted by $\\mathcal{N}(\\mu_i^*, \\rho_i^*)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343caa0",
   "metadata": {},
   "source": [
    "## IV. Smoothing for Markov chains\n",
    "\n",
    "From previous results we know that prior and the likelihoods in our Markov chain are normal distributions and thus we are left \n",
    "with the following analytical simplification task\n",
    "\n",
    "\\begin{align}\\tag{S1}\n",
    "p[x_i|x_0,x_n]\\propto \\pi[x_i]\\cdot \\lambda[x_i]\\propto \\exp\\left(-\\frac{(x_i-\\mu_i)^2}{2\\rho_i^2}\\right)\\cdot \\exp\\left(-\\frac{(x_i-\\mu_i^*)^2}{2{\\rho_i^*}^2}\\right)\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "Again this is technically demanding unless you notice that we can define two-dimensional normal distribution\n",
    "\n",
    "\\begin{align*}\n",
    "\\xi_1&\\sim \\mathcal{N}(\\mu_i, \\rho_i)\\\\\n",
    "\\xi_2&\\sim \\mathcal{N}(\\mu_i^*, \\rho_i^*)\n",
    "\\end{align*}\n",
    "\n",
    "and observe conditional distribution $\\xi_1=\\xi_2$.\n",
    "As normal distribution is closed under conditionings against linear constraints we know that the resulting distribution is again a normal distribution. \n",
    "The easiest way to derive the parameters of the normal distribution is just expression manipulation\n",
    "  \n",
    "  \n",
    "\\begin{align*}\n",
    "p[x_i| x_0, x_n]\n",
    "&\\propto\\exp\\Biggl(-\\frac{(x_i-\\mu_i)^2}{2\\rho_i^2}\\Biggr)\\cdot\n",
    "\\exp\\biggl(-\\frac{(x_{i}-\\mu_i^*)^2}{2\\rho_i^{*2}}\\biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{\\rho_i^{*2}(x_i-\\mu_i)^2+ \\rho_i^2(x_i-\\mu^*_i)^2}{2\\rho_i^2\\rho_i^{*2}}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{(\\rho_i^{*2}+\\rho_i^*)x_i^2-2(\\rho_i^{*2}\\mu_i+\\rho_i^2\\mu_i^*)x_i}{2\\rho_i^2\\rho_i^{*2}}\\Biggr)\\\\\n",
    "&\\propto\\exp\\Biggl(-\\frac{\\rho_i^{*2}+\\rho_i^*}{2\\rho_i^2\\rho_i^{*2}}\\cdot \n",
    "\\biggl(x_i^2-2\\cdot\\frac{\\rho_i^{*2}\\mu_i+\\rho_i^2\\mu_i^*}{\\rho_i^{*2}+\\rho_i^2}x_i\\biggr)\\Biggr)\\\\\n",
    "&\\propto\\exp\\left(- \n",
    "\\frac{\\biggl(x_i-\\frac{\\rho_i^{*2}\\mu_i+\\rho_i^2\\mu_i^*}{\\rho_i^{*2}+\\rho_i^2}\\biggr)^2}\n",
    "{2\\frac{\\rho_i^2\\rho_i^{*2}}{\\rho_i^{*2}+\\rho_i^2}}\\right)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "From this we can conclude that the marginal distribution $x_i|x_0,x_n$ follows indeed a normal distribution $\\mathcal{N}(\\mu, \\sigma)$ with parameters\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu&=\\frac{\\rho_i^{*2}\\mu_i+\\rho_i^2\\mu_i^*}{\\rho_i^{*2}+\\rho_i^2}\\\\\n",
    "\\sigma^2&=\\frac{\\rho_i^2\\rho_i^{*2}}{\\rho_i^{*2}+\\rho_i^2}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "**Fusion formula.**\n",
    "This result is more general. Whenever the distribution is proportional to product of two normal distribution $\\mathcal{N}(\\mu_1, \\sigma_1)$ and $\\mathcal{N}(\\mu_2, \\sigma_2)$ we get again a normal distribution $\\mathcal{N}(\\mu,\\sigma)$ with parameters\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu&=\\frac{\\sigma_2^{2}\\mu_1+\\sigma_1^2\\mu_2}{\\sigma_1^2+\\sigma_2^2}\\\\\n",
    "\\sigma^2&=\\frac{\\sigma_1^2\\sigma_2^{2}}{\\sigma_1^{2}+\\sigma_2^2}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5e2fc",
   "metadata": {},
   "source": [
    "## V. Filtering for Hidden Markov Models\n",
    "\n",
    "Let $f[x_i]=p[x_i|x_0, y_1,\\ldots, y_{i}]$ denote the desired conditional density and\n",
    "recall that the belief propagation in Hidden Markov Models is governed by the following equation\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi[x_{i+1}]=p[x_{i+1}|x_0, y_1,\\ldots, y_{i}]=\\int\\limits_{-\\infty}^\\infty p[x_{i+1},x_i|x_0, y_1,\\ldots, y_{i}]\\cdot dx_i=\\int\\limits_{-\\infty}^\\infty p[x_{i+1}|x_i]\\cdot p[x_i|x_0, y_1,\\ldots, y_{i}] \\cdot dx_i\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Then we get the relation \n",
    "\n",
    "\\begin{align}\\tag{F1}\n",
    "\\pi[x_{i+1}]=\\int\\limits_{-\\infty}^\\infty \\pi[x_{i+1}|x_i]\\cdot f[x_i] \\cdot dx_i \n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\\tag{F2}\n",
    "f[x_i]= p[x_i|x_0, y_1,\\ldots, y_{i}]\\propto p[y_i|x_i]\\cdot \\pi[x_i]\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "**First observation.** The prior $\\pi[x_1]$ is a density of normal distribution as there are no observations and the state $x_1$ is distributed according to the normal distribution as an affine transformation of $w_1$. \n",
    "\n",
    "\n",
    "**Second observation.** If the prior $\\pi[x_i]$ is proportiona to a density of normal distribution then $f[x_i]$ is also proportional a density of a normal distribution.\n",
    "\n",
    "Indeed, the formula (F2)  is analogous to the smoothing formula (S1) where the densities are of normal distributions.\n",
    "As the emittion probability $p[y_i|x_i]$ is also density of a normal distribution, the analogous reasoning assures that $f[x_i]$ is a density of a normal distribution.\n",
    "\n",
    "\n",
    "**Third observation.** If $f[x_i]$ is proportional to a density of a normal distribution then the next prior $\\pi[x_i]$ is proportional to a density of a normal distribution.\n",
    "\n",
    "Indeed, the formula (F1) is structurally identical to the prior propagation formula (P1) for Markov chains and thus the claim follows.\n",
    "\n",
    "**Corollary.** Priors $\\pi[x_i]$ and filtering $f[x_i]$ are all distributed according to normal distributions and their parameters can be found by moment matching and the fusion formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c6d88",
   "metadata": {},
   "source": [
    "## VI. Likelihood propagation for Hidden Markov Models\n",
    "\n",
    "We can copy the same techniques for propagating likelihoods in Markov chain. Let us define a reverse chain with property that the likelihood $\\lambda[x_i]$  is proportional to the prior $\\pi^*[x_i]$ in the reverse chain. \n",
    "For that we need to expand the likelihood \n",
    "\n",
    "\\begin{align}\\tag{L2}\n",
    "\\lambda[x_i]= p[y_{i+1},\\ldots, y_n| x_i]=\\int\\limits_{-\\infty}^\\infty p[y_{i+1},\\ldots, y_n| x_i, x_{i+1}]\\cdot p[x_{i+1}|x_i]\\cdot dx_{i+1}=\\int\\limits_{-\\infty}^\\infty p[y_{i+1},\\ldots, y_n| x_{i+1}]\\cdot p[x_{i+1}|x_i]\\cdot dx_{i+1}\n",
    "\\end{align}\n",
    "\n",
    "while the prior in the reverse chain can be expressed\n",
    "\n",
    "\\begin{align}\\tag{R3}\n",
    "\\pi^*[x_i]=\\int\\limits_{-\\infty}^\\infty q[x_i|x_{i+1}]\\cdot q[x_{i+1}|y_{i+1},\\ldots, y_{n}] \\cdot dx_i\\enspace.\n",
    "\\end{align}\n",
    "\n",
    "where $q[\\cdot|\\cdot]$ stands for conditional densities in the reverse chain. \n",
    "Again note that \n",
    "\n",
    "\\begin{align}\\tag{R4}\n",
    "f^*[x_{i+1}]=q[x_{i+1}|y_{i+1},\\ldots, y_{n}]\\propto  q[y_{i+1}|x_{i+1}]\\cdot q[x_{i+1}|y_{i+2}\\ldots, y_n]=q[y_{i+1}|x_{i+1}]\\cdot\\pi^*[x_{i+1}] \n",
    "\\end{align}\n",
    "\n",
    "The structural similarity between the equation pair (R3)-(R4) and  (F1)-(F2) guaranmtees that if we define \n",
    "\n",
    "\\begin{align*}\n",
    "q[x_i|x_{i+1}]\\propto p[x_{i+1}|x_i]\n",
    "\\end{align*}\n",
    "\n",
    "and define $y_n$ as the starting point of reverse Hidden Markov Model we get the desired property $\\lambda[x_i]\\propto \\pi^*[x_i]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a3fe18",
   "metadata": {},
   "source": [
    "## VII. Smoothing for Hidden Markov Models\n",
    "\n",
    "Again let us start from the observation\n",
    "\n",
    "\\begin{align*}\n",
    "p[x_i|y_1,\\ldots,y_i,\\ldots,y_n]\\propto p[y_{i},\\ldots, y_{n}|x_i, y_1,\\ldots, y_{i-1}]\\cdot p[x_i|y_1,\\ldots, y_{i-1}]\\propto p[y_i|x_i]\\cdot \\lambda[x_i] \\cdot\\pi[x_i]\n",
    "\\end{align*}\n",
    "\n",
    "As all terms in the expression are densities of normal distribution the closeness under linear constraint assures that the result is also a normal distribution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
