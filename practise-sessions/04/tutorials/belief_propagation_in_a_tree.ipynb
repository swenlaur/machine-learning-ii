{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete derivation of belief update rules for trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Formal definitions\n",
    "\n",
    "### Evidence\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-evidence-i.png' width=100%>\n",
    "\n",
    "\n",
    "* **Direct evidence** $\\varepsilon_V$ for a node $V$ is an observation $V=v_*$ that determines the **local likelihood** $\\lambda_V^*(v)=[v=v_*]$.\n",
    "* **Indirect evidence** $\\varepsilon_V$ for a node $V$ is a partial observation that determines the **local likelihood** $\\lambda_V^*(v)=\\Pr[\\varepsilon_V|V=v]$. \n",
    "\n",
    "\n",
    "As indirect evidence $\\varepsilon_V$ can be modelled by adding a new successor $V_*$ to the node $V$ with the conditional distribution $\\Pr[V_*=\\varepsilon_V|V=v]$\n",
    "determined by the local likelihood $\\lambda_V^*(v)$, we do not consider indirect evidence in the further analysis.\n",
    "For the chains, we needed to analyse the effect of the indirect evidence separately as this extra node converts a chain into a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Evidence partitioning\n",
    " \n",
    " * **Evidence** $\\mathsf{evidence}$ is the summary evidence of all nodes in the tree. \n",
    " * **Upstream evidence** $\\color{red}{\\mathsf{evidence}^+}(V)$ is the evidence of all nodes reachable through a predecessor of $V$ together with the evidence for $V$.\n",
    " * **Downstream evidence** $\\color{blue}{\\mathsf{evidence}^-}(V)$ is the evidence of all nodes succeeding $V$ together with the evidence for $V$.\n",
    "\n",
    "\n",
    " <img src = '../illustrations/belief-propagation-in-tree-evidence-ii.png' width=100%>\n",
    "\n",
    "In this figure, upstream and downstream evidence for the node $D$ are the following:\n",
    "\n",
    "\\begin{align*}\n",
    "\\color{red}{\\mathsf{evidence}^+}(D)  &= \\{\\varepsilon_A, \\varepsilon_D, \\varepsilon_I\\} \\\\\n",
    "\\color{blue}{\\mathsf{evidence}^-}(D) &= \\{\\varepsilon_D, \\varepsilon_G\\}\\enspace.\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Derivation of iterative update rules\n",
    "\n",
    "### Marginal posterior probabilities\n",
    "\n",
    " <img src = '../illustrations/belief-propagation-in-tree-marginal-posterior-i.png' width=100%>\n",
    "\n",
    "\n",
    "Mechanical application of Bayes rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "p_V(v)\n",
    "&= \\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V),\\color{blue}{\\mathsf{evidence}^-}(V)]\\\\\n",
    "&=\\frac{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v,\\color{red}{\\mathsf{evidence}^+}(V)]\n",
    "  \\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]}{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|\\color{red}{\\mathsf{evidence}^+}(V)]}\\\\\n",
    "&\\propto \\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v,\\color{red}{\\mathsf{evidence}^+}(V)]\n",
    "  \\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As direct knowledge of the state $V=v$ completely determines what happens with the next node, the knowledge $V=v,\\color{red}{\\mathsf{evidence}^+}(V)$ is equivalent to the knowledge $V=v$ and we can simplify:\n",
    "\n",
    "\\begin{align*}\n",
    "p_V(v)\n",
    "&\\propto \\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\n",
    "  \\cdot\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&\\propto \\lambda_V(v)\\cdot \\pi_V(v)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As a result, if we know the likelihood $\\lambda_V(\\cdot)$ and posterior $\\pi_V(\\cdot)$ up to a constant then we can recover the marginal posterior $p_V(\\cdot)$ through normalisation. \n",
    "Up to a constant in this context means that we can omit all factors that do not depend on the value $v$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood update for a node without evidence\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-likelihood-i.png' width=100%>\n",
    "\n",
    "Let $W_1,\\ldots,W_k$ be direct successor nodes of $V$, then the downstream evidence decomposes into $k$ classes\n",
    "\n",
    "\\begin{align*}\n",
    "\\color{blue}{\\mathsf{evidence}^-}(V)=\\color{blue}{\\mathsf{evidence}^-}(W_1)\\cup\\ldots\\cup \\color{blue}{\\mathsf{evidence}^-}(W_k)\n",
    "\\end{align*}\n",
    "\n",
    "as the node $V$ has no evidence.\n",
    "Moreover, these events are independent for a fixed $V=v$ value as they occur in different tree brances. Consequently,\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1)\\wedge\\ldots\\wedge \\color{blue}{\\mathsf{evidence}^-}(W_k)|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1)|V=v]\\cdots \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_k)|V=v]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Mechanical application of marginalisation rule to one of the terms yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_j(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_j)|V=v]\\\\\n",
    "&=\\sum_{w_j\\in W_j}\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_j)\\wedge W_j=w_j|V=v]\\\\\n",
    "&=\\sum_{w_j\\in W_j}\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_j)|W_j=w_j,V=v]\\cdot \\Pr[W_j=w_j|V=v]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "The Markov property assures that knowledge of $V=v$ is redundant when we know $W_j=w_j$. \n",
    "Consequently, we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_j(v)\n",
    "&=\\sum_{w_j\\in W_j}\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_j)|W_j=w_j]\\cdot \\Pr[W_j=w_j|V=v] \\\\\n",
    "&=\\sum_{w_j\\in W_j}\\lambda_{W_j}(w_j)M_{V\\to W_j}[v, w_j]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Representing $\\lambda_j(\\cdot)$ and $\\lambda_{W_j}(\\cdot)$ as column vectors allows us to compact the equation in matrix notation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_j&= M_{V\\to W_j} \\lambda_{W_j}\\\\\n",
    "\\lambda_V&=\\lambda_1\\otimes\\ldots\\otimes\\lambda_k\\enspace.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood update for a node with direct evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-likelihood-ii.png' width=100%>\n",
    "\n",
    "Let $V=v_*$ be a direct evidence associated with the node $V$ and let\n",
    "\n",
    "\\begin{align*}\n",
    "\\color{blue}{\\mathsf{evidence}^-_*}(V)= \\color{blue}{\\mathsf{evidence}^-}(V)\\setminus \\{V=v_*\\}\n",
    "\\end{align*}\n",
    "\n",
    "be the evidence associated with the node $V$ downstream of $V$.\n",
    "Then evidence decomposition yieds\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-_*}(V)\\wedge V=v_*|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-_*}(V)|V=v_*,V=v]\\cdot \\Pr[V=v_*|V=v]\\\\\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-_*}(V)|V=v_*]\\cdot [v_*=v]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that $\\lambda_V(v)$ is nonzero only for a single value $v_*$. \n",
    "Thus by multiplying $\\lambda_V(v)$ with a constant value $\\lambda_V(v_*)^{-1}$, we get an indicator:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\\propto [v=v_*]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Note that  $\\lambda_V(v_*)^{-1}$ depends on $v_*$ but remains constant if we consider different values of $v\\in V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood update for a node without successors\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-likelihood-iii.png' width=100%>\n",
    "\n",
    "\n",
    "The rules for updating the likelihood are applicable for nodes that do have successors.\n",
    "Hence, we must address nodes without successors explicitly. \n",
    "If the node has direct evidence $V=v_*$ then it is the entire downstream evidence $\\color{blue}{\\mathsf{evidence}^-}(V)$ and thus\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\Pr[V=v_*|V=v]\\\\\n",
    "&=[v=v_*]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "If the node does not have evidence then the entire downstream evidence $\\color{blue}{\\mathsf{evidence}^-}(V)$ is empty and thus\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_V(v)\n",
    "&=\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|V=v]\\\\\n",
    "&=\\Pr[\\mathrm{True}|V=v]\\\\\n",
    "&=1\\enspace.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update  if a node and its predecessor are without evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-prior-i.png' width=100%>\n",
    "\n",
    "Let $U$ be a predecessor node of $V$ then mechanical application of marginalisation rule yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v\\wedge U=u|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v|U=u,\\color{red}{\\mathsf{evidence}^+}(V)]\\cdot \\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(V)]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As the node $V$ has no evidence, the upstream evidence must be reachable through the predecessor node $U$.\n",
    "However, this evidence is not only $\\color{red}{\\mathsf{evidence}^+}(U)$ if $U$ has more child nodes than just $V$.\n",
    "Let $W_1,\\ldots,W_{k}$ denote the children of $U$ so that $W_k=V$. Then the upstream evidence of $V$ decomposes into up- and downstream evidence:\n",
    "\n",
    "\\begin{align*}\n",
    "\\color{red}{\\mathsf{evidence}^+}(V)=\\color{red}{\\mathsf{evidence}^+}(U)\\cup \\color{blue}{\\mathsf{evidence}^-}(W_1)\\cup \\ldots \\cup\\color{blue}{\\mathsf{evidence}^-}(W_{k-1})\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "The Markov property assures that knowledge of $\\color{red}{\\mathsf{evidence}^+}(U)$ is redundant when we know $U=u$. \n",
    "Consequently, we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\sum_{u\\in U}\\Pr[V=v|U=u,\\color{red}{\\mathsf{evidence}^+}(V)]\\cdot \\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\sum_{u\\in U}\\Pr[V=v|U=u]\\cdot \\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(U), \\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})]\\\\\n",
    "&=\\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot\\frac{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u,\\color{red}{\\mathsf{evidence}^+}(U)]\\cdot\\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(U)]}{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|\\color{red}{\\mathsf{evidence}^+}(U)]}\\\\\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u,\\color{red}{\\mathsf{evidence}^+}(U)]\\cdot\\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(U)]\n",
    "\\end{align*}\n",
    "\n",
    "where the last line follows from the fact that the denominator is a constant that does not depend on the values of $u\\in U$ and $v\\in V$.\n",
    "\n",
    "The Markov property assures that knowledge of $\\color{red}{\\mathsf{evidence}^+}(U)$ is redundant when we know $U=u$ and thus we can express\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u]\\cdot\\Pr[U=u|\\color{red}{\\mathsf{evidence}^+}(U)]\\\\\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u]\\cdot \\pi_U(u)\\\\\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\pi_U(u)\\cdot \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Let us multiply and divide the summation term by the factor $\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_{k})|U=u]$ to simplify the derivation.\n",
    "Then \n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto \\sum_{u\\in U}\\frac{M_{U\\to V}[u,v]}{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_{k})|U=u]}\\cdot \\pi_U(u)\\cdot \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u]\\cdot\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_{k})|U=u]\\\\\n",
    "&\\propto \\sum_{u\\in U}\\frac{M_{U\\to V}[u,v]}{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_{k})|U=u]}\\cdot \\pi_U(u)\\cdot \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k})|U=u]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the last equation follows from the fact that branches starting from $W_1,\\ldots, W_k$ are independent given the value $U=u$.\n",
    "\n",
    "As the node $U=u$ does not have a direct evidence $U=u_*$ linked to it, the last factor is the likelihood of $U$ by definition and we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\pi_U(u)\\cdot\\frac{\\lambda_U(u)}{\\Pr[\\color{blue}{\\mathsf{evidence}^-}(V)|U=u]}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Recall that the likelihood $\\lambda_U(u)$ splits into the product $\\lambda_1(u)\\ldots,\\lambda_k(u)$ by the likelihood update rule and thus\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\frac{\\pi_U(u)\\lambda_U(u)}{\\lambda_{k}(u)}\\\\\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\frac{p_U(u)}{\\lambda_{k}(u)}\n",
    "\\end{align*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_k(u)\n",
    "&=\\sum_{v\\in V}\\lambda_{V}(v)M_{U\\to V}[u, v]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Representing $\\pi_V(\\cdot)$ and  $p_U(\\cdot)$ as row vectors allows us to compact the equation in matrix notation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V\\propto \\frac{p_U}{\\lambda_k} M_{U\\to V} \n",
    "\\end{align*}\n",
    "\n",
    "where the division line represents element-wise division of vectors. \n",
    "If the predecessor has only one child node then the expression simplifies to\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V\\propto \\pi_U M_{U\\to V} \n",
    "\\end{align*}\n",
    "\n",
    "as expected (this is the prior update formula for chains).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update  for a node with direct evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-prior-ii.png' width=100%>\n",
    "\n",
    "Let $U$ be the predecessor node of $V$ and let $V=v_*$ be the direct evidence associated with the node $V$.\n",
    "Then the evidence decoposition yields\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\Pr[V=v|V=v_*,\\color{red}{\\mathsf{evidence}^+_*}(V)]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where $\\color{red}{\\mathsf{evidence}^+_*}(V)=\\color{red}{\\mathsf{evidence}^+}(V)\\setminus \\{V=v_*\\}$ denotes the remaining evidence upstream of $V$.\n",
    "Again the evidence $V=v_*$ is the most direct information about $V$, the remaining evidence $\\color{red}{\\mathsf{evidence}^+_*}(V)$ is irrelevant unless $\\color{red}{\\mathsf{evidence}^+_*}(V)$ directly contradicts $V=v_*$.\n",
    "In this case, nothing can be done and prior is not defined at all.\n",
    "\n",
    "Thus, we can simplify and get an indicator prior:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|V=v_*]\\\\\n",
    "&=[v=v_*]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update  if a node is without evidence  while its predecessor is with direct evidence \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-prior-iii.png' width=100%>\n",
    "\n",
    "\n",
    "In the analysis above we obtained the formula \n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto \\sum_{u\\in U}M_{U\\to V}[u,v]\\cdot \\pi_U(u)\\cdot \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u]\\\\\n",
    "\\end{align*}\n",
    "that holds for any predecessor $U$, provided that $V$ is without direct evidence.\n",
    "If the node $U$ has direct evidence $U=u_*$ then  $\\pi_U(u)\\propto[u=u_*]$ and consequently the sum reduces to a single term:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto M_{U\\to V}[u_*,v]\\cdot  \\Pr[\\color{blue}{\\mathsf{evidence}^-}(W_1), \\ldots, \\color{blue}{\\mathsf{evidence}^-}(W_{k-1})|U=u_*]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Morover, the second factor does not depend on the value of $v$ and thus we can further simplify:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&\\propto M_{U\\to V}[u_*,v]\\\\\n",
    "&\\propto \\sum_{u\\in U} \\pi_U(u)M_{U\\to V}[u,v]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "The corresponding matrix algebra formulation is\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V\n",
    "&\\propto \\pi_U M_{U\\to V}\\\\\n",
    "&\\propto \\frac{\\pi_U}{\\lambda_k}M_{U\\to V}\n",
    "\\end{align*}\n",
    "\n",
    "which is formally the same as for the non-exceptional case, although the special case formula is clearer and easier to understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior update for a node without a predecessor\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-tree-prior-iv.png' width=100%>\n",
    "\n",
    "\n",
    "The rules for updating the prior are applicable for nodes that do have predecessors.\n",
    "Hence, we must address nodes without predecessors explicitly. \n",
    "If there is a direct evidence $V=v_*$ then obviously\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\Pr[V=v|V=v_*]\\\\\n",
    "&=[v=v_*]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "and thus\n",
    "\\begin{align*}\n",
    "\\pi_V\\propto [v=v_*]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If there is no evidence then by definition\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V(v)\n",
    "&=\\Pr[V=v|\\color{red}{\\mathsf{evidence}^+}(V)]\\\\\n",
    "&=\\Pr[V=v]\\\\\n",
    "&=M_V[v]\n",
    "\\end{align*}\n",
    "\n",
    "where $M_V$ is the vector of initial probabilities. The corresponding matrix formulation is\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_V\\propto\n",
    "M_V\\enspace.\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
