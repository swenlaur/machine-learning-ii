{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Belief propagation for HMM\n",
    "\n",
    "<img src = '../illustrations/hidden-markov-model.png' width=100%>\n",
    "\n",
    "\n",
    "There are generic methods for belief propagation in any tree. However, as the Hidden Markov Model has a very specific structure, we can simplify the derivation of belief propagation rules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Evidence and local likelihood\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-hmm-i.png' width=100%>\n",
    "\n",
    "Note that the evidence in HMM can be attached only to observation nodes $Y_i$. \n",
    "Moreover, the eveidence is usually direct evidence, e.g. $Y_i=y_i$ or is missing if we failed to record $Y_i$.    \n",
    "\n",
    "Thus we can define local likelihood vectors for observed variables $Y_i$: \n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_i^*(x_i)=\n",
    "\\Pr[Y_i=y_i|X_i=x_i]=\\delta[x_i, y_i] \n",
    "\\end{align*}\n",
    "\n",
    "and constant for unobserved variables $Y_i$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_i^*(x_i)=\n",
    "\\Pr[Y_i\\in\\mathrm{supp}(Y_i)|X_i=x_i]=1\\enspace.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Prior propagation rule \n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-hmm-ii.png' width=100%>\n",
    "\n",
    "### Previous observation $y_{i-1}$ is known \n",
    "\n",
    "As the upstream evidence for $X_i$ consists of observations of $y_1, \\ldots, y_{i-1}$, we can express\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{X_i}(x_i)&=\\Pr[X_i=x_i|y_1,\\ldots, y_{i-1}]\\\\\n",
    "&=\\sum_{x_{i-1}}\\Pr[X_i=x_i, X_{i-1}=x_{i-1}|y_1,\\ldots, y_{i-1}] \\\\\n",
    "&=\\sum_{x_{i-1}}\\Pr[X_i=x_i| X_{i-1}=x_{i-1}, y_1,\\ldots, y_{i-1}]\\cdot \\Pr[X_{i-1}=x_{i-1}| y_1,\\ldots, y_{i-1}]\\\\\n",
    "&=\\sum_{x_{i-1}}\\Pr[X_i=x_i| X_{i-1}=x_{i-1}]\\cdot \\Pr[X_{i-1}=x_{i-1}| y_1,\\ldots, y_{i-1}]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "For the second term we can apply the Bayes rule:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[X_{i-1}=x_{i-1}| y_1,\\ldots, y_{i-1}] &= \\frac{\\Pr[y_{i-1}|X_{i-1}=x_{i-1}, y_1,\\ldots,y_{i-2}]\\cdot \\Pr[X_{i-1}=x_{i-1}| y_1,\\ldots,y_{i-2}]}{\\Pr[y_{i-1}| y_1,\\ldots, y_{i-2}]}\\\\\n",
    "&= \\frac{\\Pr[y_{i-1}|X_{i-1}=x_{i-1}]\\cdot \\pi_{X_{i-1}}(x_{i-1})}{\\Pr[y_{i-1}| y_1,\\ldots, y_{i-2}]}\\\\\n",
    "&=\\frac{\\lambda^*_{i-1}(x_{i-1}) \\cdot \\pi_{X_{i-1}}(x_{i-1})}{\\Pr[y_{i-1}| y_1,\\ldots, y_{i-2}]}\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "As observations $y_1, \\ldots, y_{i-1}$ are fixed and $x_{i-1}$ varies in the summation we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{X_i}(x_i)\\propto\n",
    "\\sum_{x_{i-1}}\\alpha[x_{i-1}, x_i]\\cdot \\lambda^*_{i-1}(x_{i-1}) \\cdot \\pi_{X_{i-1}}(x_{i-1})\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "### Previous observation $y_{i-1}$ is missing\n",
    "\n",
    "As the observation $y_{i-1}$ is missing, the upstream evidence consists of $y_{1}, \\ldots, y_{i-2}$ and thus\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi_{X_i}(x_i)&=\\Pr[X_i=x_i|y_1,\\ldots, y_{i-2}]\\\\\n",
    "&=\\sum_{x_{i-1}}\\Pr[X_i=x_i, X_{i-1}=x_{i-1}|y_1,\\ldots, y_{i-2}] \\\\\n",
    "&=\\sum_{x_{i-1}}\\Pr[X_i=x_i| X_{i-1}=x_{i-1}, y_1,\\ldots, y_{i-2}]\\cdot \\Pr[X_{i-1}=x_{i-1}| y_1,\\ldots, y_{i-2}]\\\\\n",
    "&=\\sum_{x_{i-1}}\\Pr[X_i=x_i| X_{i-1}=x_{i-1}]\\cdot \\Pr[X_{i-1}=x_{i-1}| y_1,\\ldots, y_{i-2}]\\\\\n",
    "&\\propto \\sum_{x_{i-1}}\\alpha[x_{i-1},x_i]\\cdot \\pi_{X_{i-1}}(x_{i-1})\\enspace. \n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Likelihood propagation rule\n",
    "\n",
    "<img src = '../illustrations/belief-propagation-in-hmm-iii.png' width=100%>\n",
    "\n",
    "\n",
    "As the downstream evidence for $X_i$ is $y_i, \\ldots, y_n$ we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_{X_i}(x)&=\\Pr[y_{i},\\ldots, y_n|X_i=x_i]\\\\\n",
    "&=\\Pr[y_{i}|X_i=x_i]\\cdot\\Pr[y_{i+1},\\ldots, y_n|X_i=x_i]\\\\\n",
    "&=\\lambda_{i}^*(x_i)\\cdot\\Pr[y_{i+1},\\ldots, y_n|X_i=x_i]\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "Now note that\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[y_{i+1},\\ldots, y_n|X_i=x_i]&=\\sum_{x_{i+1}}\\Pr[y_{i+1},\\ldots, y_n,X_{i+1}=x_{i+1}|X_i=x_i]\\\\\n",
    "&=\\sum_{x_{i+1}}\\Pr[y_{i+1},\\ldots, y_n|X_{i+1}=x_{i+1},X_i=x_i]\\cdot \\Pr[X_{i+1}=x_{i+1}|X_i=x_i]\\\\\n",
    "&=\\sum_{x_{i+1}}\\Pr[y_{i+1},\\ldots, y_n|X_{i+1}=x_{i+1}]\\cdot \\Pr[X_{i+1}=x_{i+1}|X_i=x_i]\\\\\n",
    "&=\\sum_{x_{i+1}}\\lambda_{X_{i+1}}(x_{i+1})\\cdot \\alpha[x_i,x_{i+1}]\\\\\n",
    "\\end{align*}\n",
    "\n",
    "and consequently\n",
    "\n",
    "\\begin{align*}\n",
    "\\lambda_{X_i}(x)=\n",
    "\\lambda_{i}^*(x_i)\\cdot\\sum_{x_{i+1}}\\alpha[x_i,x_{i+1}]\\cdot\\lambda_{X_{i+1}}(x_{i+1})\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Filtering and smoothing\n",
    "\n",
    "Recall that filtering is a prediction of $X_i$ given information available at the $i$-th timestep:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[X_i=x_i|y_1,\\ldots, y_i]\n",
    "&=\\frac{\\Pr[y_i|y_1,\\ldots, y_{i-1}, X_i=x_i]\\cdot \\Pr[X_i=x_i|y_1,\\ldots, y_{i-1}]}{\\Pr[y_i|y_1,\\ldots, y_{i-1}]}\\\\\n",
    "&\\propto \\Pr[y_i|X_i=x_i]\\cdot \\Pr[X_i=x_i|y_1,\\ldots, y_{i-1}]\\\\\n",
    "&\\propto \\lambda_i^*(x_i)\\cdot \\pi_{X_{i}}(x_i)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "In other words, we combine prior and local likelihood to get posterior. \n",
    "\n",
    "\n",
    "Recall that smooting is a a prediction of $X_i$ given information available after all observations $y_1,\\ldots, y_n$ are available:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[X_i=x_i|y_1,\\ldots, y_i]\n",
    "&=\\frac{\\Pr[y_i,\\ldots, y_n|y_1,\\ldots, y_{i-1}, X_i=x_i]\\cdot \\Pr[X_i=x_i|y_1,\\ldots, y_{i-1}]}{\\Pr[y_i,\\ldots, y_n|y_1,\\ldots, y_{i-1}]}\\\\\n",
    "&\\propto \\Pr[y_i,\\ldots, y_{n}|X_i=x_i]\\cdot \\Pr[X_i=x_i|y_1,\\ldots, y_{i-1}]\\\\\n",
    "&\\propto \\Pr[y_i|X_i=x_i]\\cdot\\Pr[y_{i+1},\\ldots, y_{n}|X_i=x_i]\\cdot \\Pr[X_i=x_i|y_1,\\ldots, y_{i-1}]\\\\\n",
    "&\\propto \\lambda_i^*(x_i)\\cdot\\lambda_{X_i}(x_i)\\cdot \\pi_{X_{i}}(x_i)\\enspace.\n",
    "\\end{align*}\n",
    "\n",
    "In other words, we combine prior, local likelihood and likelihood of remaining observations. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
