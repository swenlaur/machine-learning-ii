{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior maximisation for a tree\n",
    "\n",
    "Consider a tree with nodes $X_1,\\ldots, X_n$ and with some evidence. Then we can ask what is the assignment $x_1,\\ldots,x_n$ that maximises the posterior $\\Pr[x_1,\\ldots,x_n|\\text{evidence}]$ provided that the parameters of the tree are fixed.\n",
    "\n",
    "As a particular example we can consider a Hidden Markov model\n",
    "\n",
    "<img src = 'illustrations/hidden-markov-model.png' width=100%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\Pr[x_1,\\ldots, x_i|y_1,\\ldots,y_i] \\neq \\Pr[x_1,\\ldots, x_{i-1}|y_1,\\ldots,y_{i-1}]\\cdot \\Pr[x_i|\n",
    "x_{i-1},y_i]\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical example \n",
    "\n",
    "Let us maximize the probability $\\Pr[x_1,\\ldots, x_i|y_1,\\ldots,y_i]$. Our goal is to decompose the posterior into to independent parts. As the first step observe \n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[x_1,\\ldots, x_i|y_1,\\ldots,y_i] \n",
    "&= \\frac{\\Pr[x_1,\\ldots, x_{i}, y_i|y_1,\\ldots,y_{i-1}]}{\\Pr[y_i|y_1,\\ldots,y_{i-1}]} \\propto \\Pr[x_1,\\ldots, x_{i}, y_i|y_1,\\ldots,y_{i-1}]\\\\\n",
    "&=\\Pr[x_1,\\ldots, x_{i-1}|y_1,\\ldots,y_{i-1}]\\cdot \\Pr[x_i, y_i|x_1,\\ldots, x_{i-1},y_1,\\ldots,y_{i-1}]\n",
    "\\end{align*}\n",
    "\n",
    "where the last term decomposes further\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[x_i, y_i|x_1,\\ldots, x_{i-1},y_1,\\ldots,y_{i-1}] &=\n",
    "\\Pr[x_i|x_1,\\ldots, x_{i-1},y_1,\\ldots,y_{i-1}]\\cdot \\Pr[y_i|x_i, x_1,\\ldots, x_{i-1},y_1,\\ldots,y_{i-1}]\n",
    "\\end{align*}\n",
    "\n",
    "To continue further we need to remove irrelevant knowledge form the probability expressions\n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[x_i&|x_1,\\ldots, x_{i-1},y_1,\\ldots,y_{i-1}] = \\Pr[x_i|x_{i-1}]\\\\\n",
    "\\Pr[y_i&|x_i, x_1,\\ldots, x_{i-1},y_1,\\ldots,y_{i-1}] = \\Pr[y_i|x_i]\n",
    "\\end{align*}\n",
    "\n",
    "As a result, we can express \n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[x_1,\\ldots, x_i|y_1,\\ldots,y_i] = \\Pr[x_1,\\ldots, x_{i-1}|y_1,\\ldots,y_{i-1}] \\cdot \\Pr[x_i|x_{i-1}]\\cdot \\Pr[y_i|x_i]\n",
    "\\end{align}\n",
    "\n",
    "where the first term is recirsively the same optimisation task with fewer observations and the next two terms are things we can compute. \n",
    "Visually, we are left with the following maximisation task  \n",
    "\n",
    "<img src = 'illustrations/tree-max-i.png' width=100%>\n",
    "\n",
    "where the red part corresponds to optimal assignments with fixed end state $x_{i-1}$ and blue parts correspond to the second and the third term in the minimisation task.\n",
    "\n",
    "The oprimisation task becomes more obvious if we consider logarithm of the optimisation tasks.\n",
    "Then out goal is to find a tree that passes all nodes and has the minimal weight.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical example: HMM with two hidden states \n",
    "\n",
    "Let us consider a model where there are two hidden states $0$ and $1$. \n",
    "Let the the vector of initial probabilities and transition matrix given as follows\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\beta}=\n",
    "\\begin{pmatrix}\n",
    "50\\%\\\\\n",
    "50\\%\n",
    "\\end{pmatrix}\n",
    "\\qquad\n",
    "\\boldsymbol{\\alpha}=\n",
    "\\begin{pmatrix}\n",
    "90\\% & 10\\%\\\\\n",
    "10\\% & 90\\%\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "i.e. both states are initially equally probable and a state flip occurs with probability 10\\%.\n",
    "Let the observable outcomes be also zero and one and the emission matrix be \n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\delta}=\n",
    "\\begin{pmatrix}\n",
    "80\\% & 20\\%\\\\\n",
    "20\\% & 80\\%\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "i.e. the observation is flipped with 20\\% probability. Let us now analyze the case where we observe the sequence $0, 1, 0$. \n",
    "\n",
    "For a particular run of hidden values we can always assign a probability. For brevity we omit $\\boldsymbol{\\alpha},\\boldsymbol{\\beta},\\boldsymbol{\\delta}$ from the list of knowns.\n",
    "Then \n",
    "\n",
    "\\begin{align*}\n",
    "\\Pr[x_1x_2x_3|y=010]=\\frac{\\Pr[y=010|x_1x_2x_3]\\cdot\\Pr[x_1x_2x_3]}{\\Pr[y=010]}\\propto\\Pr[y=010|x_1x_2x_3]\\cdot\\Pr[x_1x_2x_3]\n",
    "\\end{align*}\n",
    "\n",
    "as the probability $\\Pr[y=010]$ does not change if we consider different guesses for $x$.\n",
    "Now \n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[y=010|x_1x_2x_3]=\\Pr[x_1\\to 0]\\cdot\\Pr[x_2\\to 1]\\cdot \\Pr[x_3\\to 0]\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align}\n",
    "\\Pr[x_1x_2x_3]=\\Pr[x_1]\\cdot\\Pr[x_1\\to x_2]\\cdot \\Pr[x_2\\to x_3]\\enspace.\n",
    "\\end{align}\n",
    " \n",
    "Now if we consider log-posterior we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\log \\Pr[x_1x_2x_3|y=010]=c +\\log\\Pr[x_1] + \\log\\Pr[x_1\\to 0]+ \\log\\Pr[x_1\\to x_2]+\\log\\Pr[x_2\\to 1]+ \\log\\Pr[x_2\\to x_3] + \\log\\Pr[x_3\\to 0]\n",
    "\\end{align*}\n",
    "\n",
    "We can easily try out all $8$ combinations of $x_1x_2x_3$ and choos the one with the higest log-posterior. However, we can split the formula in two halves and maximise them independently\n",
    "\n",
    "\\begin{align*}\n",
    "\\max_{x_2,x_3}\\left(\\max_{x_1}\\log\\Pr[x_1] + \\log\\Pr[x_1\\to 0]+ \\log\\Pr[x_1\\to x_2]+\\log\\Pr[x_2\\to 0]\\right)+ \\log\\Pr[x_2\\to x_3] + \\log\\Pr[x_3\\to 1]\n",
    "\\end{align*}\n",
    "\n",
    "As a result, we need to know optimal assignments that correspond to $x_2=0$ and $x_2=1$ inside the brakets. Obviously, we can apply the same trick to find the solution inside the brakets. \n",
    "Therfore, we can solve the entire optimisation task iteratively by asking the following questions\n",
    "\n",
    "* What is the optimal assignment for observation $y=0$ that ends with $x_1$ and what is the corresponding probability?\n",
    "\n",
    "* What is the optimal assignment for observation $y=01$ that ends with $x_2$ and what is the corresponding probability?\n",
    "\n",
    "* What is the optimal assignment for observation $y=010$ that ends with $x_3$ and what is the corresponding probability?\n",
    "\n",
    "* What which of those two final assignments has the highest probability?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
