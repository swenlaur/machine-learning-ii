{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate linear regression\n",
    "\n",
    "Let us consider a probabilistic model \n",
    "\n",
    "\\begin{align*}\n",
    "  y_i=ax_i+b+\\varepsilon_i,\\qquad \\varepsilon_i\\sim\\mathcal{N}(0,\\sigma)\n",
    "\\end{align*}\n",
    "\n",
    "for a fixed set of observations $x_1,\\ldots,x_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Probabilistic model and likelihood \n",
    "\n",
    "First observe\n",
    "\n",
    "\\begin{align*}\n",
    "y_i=ax_i+b+\\varepsilon_i \\qquad \\Longleftrightarrow \\qquad \\varepsilon_i= y_i-ax_i-b\n",
    "\\end{align*}\n",
    "\n",
    "As the leading coefficent in front of $y_i$ is one and the rest is constant, the densities are equal:\n",
    "\n",
    "\\begin{align*}\n",
    "p[y_i|x_i, a,b,\\sigma]&=p[\\varepsilon_i= y_i-ax_i-b|a,b,\\sigma]\n",
    "\\end{align*}\n",
    "\n",
    "Thus\n",
    "\\begin{align*}\n",
    "p[y_i|x_i, a,b,\\sigma]&=\n",
    "\\frac{1}{\\sqrt{2\\pi} \\sigma}\\cdot\\exp\\left(-\\frac{(y_i-ax_i-b)^2}{2\\sigma^2}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "As $\\varepsilon_i$ are indpendent we get \n",
    "\n",
    "\\begin{align*}\n",
    "  p[\\mathbf{y}|\\mathbf{x},a,b, \\sigma]&=\\prod_{i=1}^n p[y_i|x_i, a, b, \\sigma]\\\\\n",
    "  &=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi} \\sigma}\\cdot\\exp\\left(-\\frac{(y_i-ax_i-b)^2}{2\\sigma^2}\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Maximum likelihood principle\n",
    "\n",
    "We can establish log-likelihood of the data\n",
    "\n",
    "\\begin{align*}\n",
    "\\log p[\\mathbf{y}|\\mathbf{x},a,b,\\sigma]&= const-n\\log(\\sigma)-\\sum_{i=1}^n\\frac{(y_i-ax_i-b)^2}{2\\sigma^2}\n",
    "\\end{align*}\n",
    "\n",
    "and apply the maximum likelihood principle, i.e., to pick a model that maximises the likelihood of the data.\n",
    "\n",
    "\n",
    "**Bayesian thinking and internal consistency:** In most cases, one can attach non-informative prior to all models and choose the one with the highest posterior probability and get the same model. \n",
    "Using maximum likelihood principle directly is better as defining a non-informative prior over models depends on the parametrisation, i.e., while using the maximum likelihood principle is independent of the model parametrisation.   \n",
    "\n",
    "If we fix $\\sigma$ them the maximisation task is equivalent to minimising \n",
    "\n",
    "\\begin{align*}\n",
    "F(a,b)=\\frac{1}{n}\\cdot\\sum_{i=1}^n(y_i-ax_i-b)^2\n",
    "\\end{align*}\n",
    "\n",
    "In other words we need to minimise the mean-squared error to get the optimal solution. \n",
    "\n",
    "\n",
    "If we fix $a$ and $b$ then the maximisation task is equivalent to minimising\n",
    "\n",
    "\\begin{align*}\n",
    "F(\\sigma)= n\\log \\sigma+\\frac{1}{2\\sigma^2}\\cdot\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "where $\\hat{y}_i=ax_i+b$ is the prediction. By equating the first derivative to zero we get\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial F(\\sigma)}{\\partial \\sigma}= \\frac{n}{\\sigma}- \\frac{1}{\\sigma^3}\\cdot\\sum_{i=1}^n(y_i-\\hat{y}_i)^2=0\n",
    "\\end{align*}\n",
    "\n",
    "which implies\n",
    "\n",
    "\\begin{align*}\n",
    "\\sigma^2= \\frac{1}{n}\\cdot\\sum_{i=1}^n(y_i-\\hat{y}_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "The latter is equivalent to fitting normal distribution with zero mean to residuals $y_i-\\hat{y}_i$.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
